{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7bfc30f6-470a-402c-b0a8-bab8789cd60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "from pycldf.dataset import Dataset\n",
    "import pandas as pd\n",
    "from numpy.linalg import norm\n",
    "from scipy import spatial\n",
    "from zipfile import ZipFile as zf\n",
    "import gram2vec.lang2vec.lang2vec as l2v\n",
    "import pickle\n",
    "from helpers import to_iso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6d37321",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reads in gold standard MT scores for languages and finds the ranking\n",
    "import scipy.stats as ss\n",
    "import pandas as pd\n",
    "POS_XLMR = \"./golds/xpos_scores.csv\"\n",
    "\n",
    "data = pd.read_csv(POS_XLMR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a52453c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = pickle.load(open(\"training-data/POS_gram_golds_no_ties.pkl\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "441a3460",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Loading egg at /Users/CitronVert/opt/anaconda3/envs/lang2vec-clean/lib/python3.12/site-packages/lang2vec-1.1.6-py3.12.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting langcodes\n",
      "  Downloading langcodes-3.3.0-py3-none-any.whl.metadata (29 kB)\n",
      "Using cached langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
      "Installing collected packages: langcodes\n",
      "Successfully installed langcodes-3.3.0\n"
     ]
    }
   ],
   "source": [
    "!pip install langcodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "abfbec0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "from langcodes import Language\n",
    "\n",
    "def to_iso(lang):\n",
    "    return Language.make(lang).to_alpha3()\n",
    "\n",
    "pos = pickle.load(open(\"training-data/POS_gram_golds_no_ties.pkl\", 'rb'))\n",
    "gold_langs = set(pos.keys())\n",
    "POS_XLMR = \"./golds/xpos_scores.csv\"\n",
    "data = pd.read_csv(POS_XLMR)\n",
    "\n",
    "data.columns = [to_iso(col) if col != \"lang_train\" else col for col in data.columns]# Convert contents of the first column from 2-letter to 3-letter ISO codes\n",
    "\n",
    "data['lang_train'] = data['lang_train'].apply(lambda x: to_iso(x) if pd.notnull(x) else x)\n",
    "filtered_data = data[data['lang_train'].isin(gold_langs)]\n",
    "\n",
    "# Filter out columns with language codes not present in gold_langs\n",
    "filtered_data = filtered_data[filtered_data.columns[0:1].tolist() + [col for col in filtered_data.columns[1:] if col in gold_langs]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e550b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_data = filtered_data[filtered_data['lang_train'].isin(filtered_data.columns)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c5be8bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "languages = list(filtered_data['lang_train'].unique()) + [\"lang_train\"]\n",
    "filtered_columns = [col for col in filtered_data.columns if col in languages]\n",
    "filtered_data = filtered_data[filtered_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2b118fe4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lang_train</th>\n",
       "      <th>bel</th>\n",
       "      <th>gle</th>\n",
       "      <th>hun</th>\n",
       "      <th>hye</th>\n",
       "      <th>lit</th>\n",
       "      <th>mar</th>\n",
       "      <th>tam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bel</td>\n",
       "      <td>98.282538</td>\n",
       "      <td>59.220108</td>\n",
       "      <td>76.205972</td>\n",
       "      <td>87.359349</td>\n",
       "      <td>89.862254</td>\n",
       "      <td>85.276074</td>\n",
       "      <td>85.448916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>gle</td>\n",
       "      <td>79.140645</td>\n",
       "      <td>86.046512</td>\n",
       "      <td>77.143951</td>\n",
       "      <td>71.750060</td>\n",
       "      <td>76.381656</td>\n",
       "      <td>79.141104</td>\n",
       "      <td>78.095975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>hun</td>\n",
       "      <td>77.310767</td>\n",
       "      <td>59.830867</td>\n",
       "      <td>97.339204</td>\n",
       "      <td>85.683505</td>\n",
       "      <td>79.674114</td>\n",
       "      <td>87.116564</td>\n",
       "      <td>82.739938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>hye</td>\n",
       "      <td>91.918561</td>\n",
       "      <td>62.426591</td>\n",
       "      <td>85.729326</td>\n",
       "      <td>97.031362</td>\n",
       "      <td>89.005543</td>\n",
       "      <td>83.435583</td>\n",
       "      <td>86.919505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>lit</td>\n",
       "      <td>89.145641</td>\n",
       "      <td>60.104925</td>\n",
       "      <td>76.885528</td>\n",
       "      <td>84.055542</td>\n",
       "      <td>96.128003</td>\n",
       "      <td>82.822086</td>\n",
       "      <td>79.140867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>mar</td>\n",
       "      <td>65.400949</td>\n",
       "      <td>50.140944</td>\n",
       "      <td>69.410413</td>\n",
       "      <td>74.431410</td>\n",
       "      <td>75.919704</td>\n",
       "      <td>88.957055</td>\n",
       "      <td>73.529412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>tam</td>\n",
       "      <td>76.942293</td>\n",
       "      <td>58.237413</td>\n",
       "      <td>69.783691</td>\n",
       "      <td>76.466363</td>\n",
       "      <td>77.767512</td>\n",
       "      <td>79.141104</td>\n",
       "      <td>85.642415</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   lang_train        bel        gle        hun        hye        lit  \\\n",
       "2         bel  98.282538  59.220108  76.205972  87.359349  89.862254   \n",
       "20        gle  79.140645  86.046512  77.143951  71.750060  76.381656   \n",
       "28        hun  77.310767  59.830867  97.339204  85.683505  79.674114   \n",
       "29        hye  91.918561  62.426591  85.729326  97.031362  89.005543   \n",
       "37        lit  89.145641  60.104925  76.885528  84.055542  96.128003   \n",
       "40        mar  65.400949  50.140944  69.410413  74.431410  75.919704   \n",
       "56        tam  76.942293  58.237413  69.783691  76.466363  77.767512   \n",
       "\n",
       "          mar        tam  \n",
       "2   85.276074  85.448916  \n",
       "20  79.141104  78.095975  \n",
       "28  87.116564  82.739938  \n",
       "29  83.435583  86.919505  \n",
       "37  82.822086  79.140867  \n",
       "40  88.957055  73.529412  \n",
       "56  79.141104  85.642415  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f9bd45b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_gold_xpos(data):\n",
    "    indices = data.columns \n",
    "    indices = list(indices[1:]) # creates final list of languages\n",
    "    langs_ranked_ties = {}\n",
    "    # creates a ranking for each language (with the language itself removed from consideration)\n",
    "    for language in indices: \n",
    "        values = list(data[language])\n",
    "        values = [-float(a) for a in values]\n",
    "        ranked =  ss.rankdata(values, method = \"ordinal\") #ranks remaining languages (ties are assigned the minimum ranking that would be assigned if no ties were presen)\n",
    "        langs_ranked_ties[language] = (indices, ranked)\n",
    "    return langs_ranked_ties\n",
    "\n",
    "def make_xpos_rank(data, lang):\n",
    "    remove = [lang]\n",
    "    data = data.drop(remove,axis = 1) #columns that match undesirable language codes\n",
    "    data = data.drop(data.loc[data[\"lang_train\"].isin(remove)].index) # removes rows that match undesirable language codes\n",
    "    indices = data.columns \n",
    "    indices = list(indices[1:]) # creates final list of languages\n",
    "    rankings = {}\n",
    "    for lang in indices:\n",
    "        values = list(data[lang])\n",
    "        values = [-float(a) for a in values]\n",
    "        ranked =  ss.rankdata(values, method = \"ordinal\") #ranks remaining languages (ties are assigned the minimum ranking that would be assigned if no ties were presen)\n",
    "        ranked = [i - 1 for i in ranked]\n",
    "        rankings[lang] = ranked\n",
    "    return (indices, rankings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cca641a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "gram_golds = make_gold_xpos(filtered_data)\n",
    "with open(\"./training-data/POS_XLMR_gram_golds_no_ties.pkl\", 'wb') as f:\n",
    "    pickle.dump(gram_golds, f)\n",
    "\n",
    "training = {}\n",
    "for language in filtered_data.columns[1:]:\n",
    "    ranking = make_xpos_rank(filtered_data, language)\n",
    "    training[language] = ranking\n",
    "with open(\"./training-data/POS_XLMR_gram_ranked_train_no_ties.pkl\", 'wb') as f:\n",
    "    pickle.dump(training, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ba4dd6f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bel': (['gle', 'hun', 'hye', 'lit', 'mar', 'tam'],\n",
       "  {'gle': [0, 3, 1, 2, 5, 4],\n",
       "   'hun': [2, 0, 1, 3, 5, 4],\n",
       "   'hye': [5, 1, 0, 2, 4, 3],\n",
       "   'lit': [4, 2, 1, 0, 5, 3],\n",
       "   'mar': [4, 1, 2, 3, 0, 5],\n",
       "   'tam': [4, 2, 0, 3, 5, 1]}),\n",
       " 'gle': (['bel', 'hun', 'hye', 'lit', 'mar', 'tam'],\n",
       "  {'bel': [0, 3, 1, 2, 5, 4],\n",
       "   'hun': [3, 0, 1, 2, 5, 4],\n",
       "   'hye': [1, 2, 0, 3, 5, 4],\n",
       "   'lit': [1, 3, 2, 0, 5, 4],\n",
       "   'mar': [2, 1, 3, 4, 0, 5],\n",
       "   'tam': [2, 3, 0, 4, 5, 1]}),\n",
       " 'hun': (['bel', 'gle', 'hye', 'lit', 'mar', 'tam'],\n",
       "  {'bel': [0, 3, 1, 2, 5, 4],\n",
       "   'gle': [3, 0, 1, 2, 5, 4],\n",
       "   'hye': [1, 5, 0, 2, 4, 3],\n",
       "   'lit': [1, 4, 2, 0, 5, 3],\n",
       "   'mar': [1, 4, 2, 3, 0, 5],\n",
       "   'tam': [2, 4, 0, 3, 5, 1]}),\n",
       " 'hye': (['bel', 'gle', 'hun', 'lit', 'mar', 'tam'],\n",
       "  {'bel': [0, 2, 3, 1, 5, 4],\n",
       "   'gle': [3, 0, 2, 1, 5, 4],\n",
       "   'hun': [3, 1, 0, 2, 5, 4],\n",
       "   'lit': [1, 4, 2, 0, 5, 3],\n",
       "   'mar': [2, 4, 1, 3, 0, 5],\n",
       "   'tam': [1, 4, 2, 3, 5, 0]}),\n",
       " 'lit': (['bel', 'gle', 'hun', 'hye', 'mar', 'tam'],\n",
       "  {'bel': [0, 2, 3, 1, 5, 4],\n",
       "   'gle': [3, 0, 2, 1, 5, 4],\n",
       "   'hun': [3, 2, 0, 1, 5, 4],\n",
       "   'hye': [1, 5, 2, 0, 4, 3],\n",
       "   'mar': [2, 4, 1, 3, 0, 5],\n",
       "   'tam': [2, 4, 3, 0, 5, 1]}),\n",
       " 'mar': (['bel', 'gle', 'hun', 'hye', 'lit', 'tam'],\n",
       "  {'bel': [0, 3, 4, 1, 2, 5],\n",
       "   'gle': [4, 0, 3, 1, 2, 5],\n",
       "   'hun': [4, 2, 0, 1, 3, 5],\n",
       "   'hye': [1, 5, 2, 0, 3, 4],\n",
       "   'lit': [1, 5, 3, 2, 0, 4],\n",
       "   'tam': [2, 5, 3, 0, 4, 1]}),\n",
       " 'tam': (['bel', 'gle', 'hun', 'hye', 'lit', 'mar'],\n",
       "  {'bel': [0, 3, 4, 1, 2, 5],\n",
       "   'gle': [4, 0, 3, 1, 2, 5],\n",
       "   'hun': [4, 2, 0, 1, 3, 5],\n",
       "   'hye': [1, 5, 2, 0, 3, 4],\n",
       "   'lit': [1, 4, 3, 2, 0, 5],\n",
       "   'mar': [2, 5, 1, 3, 4, 0]})}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f33f0a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_df = data.drop(columns=['lang_train']).apply(pd.to_numeric)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "878f7894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column with the greatest span of values: cat\n",
      "cat: Min Value: 25.72064229057845 (Index: san), Max Value: 95.65196362932868 (Index: spa)\n"
     ]
    }
   ],
   "source": [
    "df = data.copy()\n",
    "\n",
    "for column in df.columns:\n",
    "    # Set entries to null where the value in 'lang_train' matches the column name\n",
    "    df.loc[df['lang_train'] == column, column] = np.nan\n",
    "\n",
    "numeric_df = df.drop(columns=['lang_train']).apply(pd.to_numeric)\n",
    "\n",
    "\n",
    "ranges = numeric_df.max(skipna=True) - numeric_df.min(skipna=True)# Find the column with the maximum range\n",
    "column_with_max_span = ranges.idxmax()\n",
    "\n",
    "print(\"Column with the greatest span of values:\", column_with_max_span)\n",
    "min_val = numeric_df[column_with_max_span].min()\n",
    "max_val = numeric_df[column_with_max_span].max()\n",
    "min_index = data.loc[data[column_with_max_span] == min_val, \"lang_train\"].iloc[0]\n",
    "max_index = data.loc[data[column_with_max_span] == max_val, \"lang_train\"].iloc[0]\n",
    "print(f\"{column_with_max_span}: Min Value: {min_val} (lang: {min_index}), Max Value: {max_val} (lang: {max_index})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62fc35d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = data[:54] #cuts off the top 3 data from the bottom of the csv (MT)\n",
    "# data = data[:31] # (DEP)\n",
    "# data = data[:60] # (POS)\n",
    "# data = data[:54] #(EL)\n",
    "\n",
    "\n",
    "def generate_rankings(data, remove):\n",
    "    remove = list(set(remove) & set(data.columns))\n",
    "    data = data.drop(remove, axis = 1) # drop test languages (columns) that match undesirable language codes\n",
    "    data = data.drop(data.loc[data['v extra v \\ trg >>'].isin(remove)].index) # removes rows that match undesirable language codes\n",
    "    indices = data.columns \n",
    "    indices = list(indices[1:]) # creates final list of languages\n",
    "    langs_ranked_ties = {}\n",
    "    # creates a ranking for each language (with the language itself removed from consideration)\n",
    "    for language in indices: \n",
    "        values = list(data[language])\n",
    "        values = [-float(a) for a in values]\n",
    "        ranked =  ss.rankdata(values, method = \"ordinal\") #ranks remaining languages (ties are assigned the minimum ranking that would be assigned if no ties were presen)\n",
    "        langs_ranked_ties[language] = (indices, ranked)\n",
    "    return langs_ranked_ties\n",
    "\n",
    "\n",
    "def generate_ranking(data, language, r):\n",
    "    # remove.append(language)\n",
    "    remove = r.copy()\n",
    "    remove.append(language)\n",
    "    remove = list(set(remove) & set(data.columns))\n",
    "    data = data.drop(remove,axis = 1) #columns that match undesirable language codes\n",
    "    data = data.drop(data.loc[data['v extra v \\ trg >>'].isin(remove)].index) # removes rows that match undesirable language codes\n",
    "    indices = data.columns \n",
    "    indices = list(indices[1:]) # creates final list of languages\n",
    "    langs_ranked_ties = {}\n",
    "    # creates a ranking for each language (with the language itself removed from consideration)\n",
    "    rankings = {}\n",
    "    for lang in indices:\n",
    "        values = list(data[lang])\n",
    "        values = [-float(a) for a in values]\n",
    "        ranked =  ss.rankdata(values, method = \"ordinal\") #ranks remaining languages (ties are assigned the minimum ranking that would be assigned if no ties were presen)\n",
    "        ranked = [i - 1 for i in ranked]\n",
    "        rankings[lang] = ranked\n",
    "    return (indices, rankings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1340c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.rename(columns={'v pivot v \\ test >>':'v extra v \\ trg >>'}) #stupid POS/EL inconsistency \n",
    "# data = data.rename(columns={'Unnamed: 0':'v extra v \\ trg >>'}) #stupid POS/EL inconsistency \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263a9cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finds the intersection of the grambank languages and task languages\n",
    "# finds the rankings for each language with non-intersection languages removed and saves to a pickle file\n",
    "\n",
    "import pickle\n",
    "import gram2vec.lang2vec.lang2vec as l2v\n",
    "\n",
    "gram = l2v.available_gram()\n",
    "\n",
    "\n",
    "col_langs = to_iso(list(data.columns)[1:])\n",
    "print(\"there  were {l} test languages originally. Those languages were {col}.\".format(l = len(col_langs), col = col_langs))\n",
    "\n",
    "lang = list(data['v extra v \\ trg >>'])\n",
    "lang = to_iso(lang)\n",
    "missing = [a for a in col_langs if not a in lang]\n",
    "print(\"there were  {l} languages excluded for not being present in ranking data. Those languages were {missing}.\".format(l = len(missing), missing = missing))\n",
    "\n",
    "\n",
    "data['v extra v \\ trg >>'] = lang #reassigns ranking langs to ISO-3 codes\n",
    "data.columns = ['v extra v \\ trg >>'] + col_langs #reassigns test languages to ISO-3 codes\n",
    "data = data.drop(missing,axis = 1) #drop test languages (columns) that aren't present in ranking languages (for now)\n",
    "\n",
    "col_langs = list(data.columns)[1:]  #reassigns column headers to remaining test languages\n",
    "print(\"After dropping missing languages, there are {l} test languages remaining. Those languages were {col}.\".format(l = len(col_langs), col = col_langs))\n",
    "\n",
    "\n",
    "common_test = (set(gram) & set(col_langs)) # final set of test languages (in grambank AND in ranking languages)\n",
    "\n",
    "total_langs = lang + col_langs\n",
    "\n",
    "common= (set(gram) & set(total_langs))\n",
    "remove = list(set(total_langs) ^ common) # languages that should be removed (not in grambank)\n",
    "print(\"there were {l} total languages excluded for not being present in grambank. Those languages were {missing}.\".format(l = len(remove), missing = remove))\n",
    "\n",
    "training = {}\n",
    "gram_golds = generate_rankings(data, remove)\n",
    "with open(\"./training-data/MT_gram_golds_no_ties.pkl\", 'wb') as f:\n",
    "    pickle.dump(gram_golds, f)\n",
    "\n",
    "for language in common_test:\n",
    "    ranking = generate_ranking(data, language, remove)\n",
    "    training[language] = ranking\n",
    "with open(\"./training-data/MT_ranked_train_no_ties.pkl\", 'wb') as f:\n",
    "    pickle.dump(training, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loads in list of languages that have precomputed distances in lang2vec\n",
    "with open(\"./gram2vec/lang2vec/data/distances/distances_languages.txt\") as f:\n",
    "    dist_langs = f.read()\n",
    "dist_langs =  dist_langs.split(\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1268fb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract ISO6393 codes from... somewhere\n",
    "with open('/Users/CitronVert/Downloads/resourcemap.json') as f:\n",
    "  \n",
    "# returns JSON object as \n",
    "# a dictionary\n",
    "    data = json.load(f)\n",
    "\n",
    "def get_iso6393(identifiers):\n",
    "    for identifier in identifiers:\n",
    "        if identifier.get(\"type\") == \"iso639-3\":\n",
    "            return identifier.get(\"identifier\")\n",
    "    return None\n",
    "\n",
    "# Use list comprehension to extract the required fields\n",
    "glotto2iso = {\n",
    "        resource.get(\"id\"): get_iso6393(resource.get(\"identifiers\", []))\n",
    "        for resource in data.get(\"resources\", [])\n",
    "        if get_iso6393(resource.get(\"identifiers\", [])) is not None\n",
    "    }\n",
    "\n",
    "iso2glotto = {v: k for k, v in glotto2iso.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6265a4a-d4d6-4b23-9060-0c957307f74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace language codes in the vector data with ISO codes\n",
    "\n",
    "from pycldf.dataset import Dataset\n",
    "# metadata = \"/Users/CitronVert/Desktop/grambank-grambank-analysed-fcf971a/grambank/cldf/StructureDataset-metadata.json\"\n",
    "imputed = \"/Users/CitronVert/Desktop/NALA/grambank-grambank-analysed-fcf971a/R_grambank/output/GB_wide/GB_wide_imputed_binarized.tsv\"\n",
    "v = pd.read_csv(imputed, sep='\\t')\n",
    "for lang in v[\"Language_ID\"]:\n",
    "    if lang in glotto2iso.keys():\n",
    "        v.loc[ v[\"Language_ID\"] == lang, \"Language_ID\"] = glotto2iso[lang]\n",
    "    else:\n",
    "        v.drop(v[v['Language_ID'] == lang].index, inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd80415b",
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = list(v.columns[1:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a47c4d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = \"/Users/CitronVert/Downloads/feature_grouping_with_alternatives.csv\"\n",
    "gram_categories = pd.read_csv(categories)\n",
    "cats = {}\n",
    "groups = {}\n",
    "for feat in feats:\n",
    "    if not len(feat) == 5:\n",
    "        feat = feat[:5]\n",
    "    domain = gram_categories.loc[gram_categories[\"Feature_ID\"] == feat, 'Main_domain'].item()\n",
    "    fine = gram_categories.loc[gram_categories[\"Feature_ID\"] == feat, 'Finer_grouping'].item()\n",
    "    if fine in groups.keys():\n",
    "        groups[fine].append(feat)\n",
    "    else:\n",
    "        groups[fine] = [feat]\n",
    "    if domain in cats.keys():\n",
    "        cats[domain].append(feat)\n",
    "    else:\n",
    "        cats[domain] = [feat]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6de78163-1e92-4381-93cf-ea93430c489f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data: pandas array with grambank vector data (imputed and converted to iso codes)\n",
    "#iso: string ISO code for a language\n",
    "#reuturns the grambank feature vector\n",
    "def get_vector(data, iso, sub_feats=None):\n",
    "    uriel = np.array(l2v.get_features(iso, \"syntax_knn\", sub_feats=sub_feats)[iso])\n",
    "    # gram = np.array(data[data['Language_ID'] == iso].values.tolist()[0][1:])\n",
    "    # concat = np.concatenate((uriel, gram))\n",
    "    return uriel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e54db655",
   "metadata": {},
   "outputs": [],
   "source": [
    "#finds the languages that are present in both lang2vec and grambank and reports the length\n",
    "with open(\"/Users/CitronVert/Desktop/NALA/bankrank/gram2vec/lang2vec/data/distances/intersection_langs.txt\", 'r') as file:\n",
    "    intersection = file.read()\n",
    "intersection = intersection.split(\",\")\n",
    "d =len(intersection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "209cec48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in csv\n",
    "fine_cat =\"/Users/CitronVert/Desktop/NALA/bankrank data/fine_categorization.csv\"\n",
    "cats = pd.read_csv(fine_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb3ea497",
   "metadata": {},
   "outputs": [],
   "source": [
    "np_order = cats[\"Uriel\"][12].split(\",\")\n",
    "clause = cats[\"Uriel\"][11].split(\",\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "90ecde2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "41\n"
     ]
    }
   ],
   "source": [
    "np_order=[a.strip() for a in np_order]\n",
    "clause = [a.strip() for a in clause]\n",
    "\n",
    "print(len(np_order))\n",
    "print(len(clause))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_vecs = [get_vector(v, iso) for iso in intersection]\n",
    "sub_np_feat_vecs = [get_vector(v, iso, np_order) for iso in intersection]\n",
    "sub_clause_feat_vecs = [get_vector(v, iso, clause) for iso in intersection]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 0.,\n",
       "       0., 1., 0., 1., 1., 1., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 1.,\n",
       "       0., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0., 1., 1.,\n",
       "       0., 0.])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_np_feat_vecs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 1., 0., 1., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0.,\n",
       "       0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "       0., 0., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 0., 0., 1., 0., 0.,\n",
       "       1., 1., 1., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0.,\n",
       "       1., 0., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1.,\n",
       "       0.])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat_vecs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "08f32033",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_arr = np.stack(feat_vecs)\n",
    "sub_np_feat_arr =  np.stack(sub_np_feat_vecs)\n",
    "sub_clause_feat_arr =  np.stack(sub_clause_feat_vecs)\n",
    "\n",
    "# print(sub_np_feat_arr.shape)\n",
    "# print(sub_clause_feat_arr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "401b7e18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "import sklearn.metrics.pairwise as sk\n",
    "import scipy.sparse as sparse\n",
    "\n",
    "cosine_similarities = sk.cosine_similarity(feat_arr)\n",
    "knn = sparse.csr_matrix(1-cosine_similarities)\n",
    "sparse.save_npz(\"./URIEL_knn_distances.npz\", knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse as sparse\n",
    "\n",
    "np = sparse.load_npz(\"/Users/CitronVert/Desktop/NALA/bankrank/gram2vec/lang2vec/data/distances/URIEL_knn_distances_np_order_ablation.npz\")\n",
    "clause = sparse.load_npz(\"/Users/CitronVert/Desktop/NALA/bankrank/gram2vec/lang2vec/data/distances/URIEL_knn_distances_clause_ablation.npz\")\n",
    "reg = sparse.load_npz(\"/Users/CitronVert/Desktop/NALA/bankrank/gram2vec/lang2vec/data/distances/URIEL_knn_distances.npz\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t-2.220446049250313e-16\n",
      "  (0, 1)\t0.5890025317366068\n",
      "  (0, 2)\t0.05370837673721851\n",
      "  (0, 3)\t0.4856555001263603\n",
      "  (0, 4)\t0.3043344070000654\n",
      "  (0, 5)\t0.48550424457247354\n",
      "  (0, 6)\t0.3617152614957745\n",
      "  (0, 7)\t0.4645503299035949\n",
      "  (0, 8)\t0.6284197321912309\n",
      "  (0, 9)\t0.41666666666666663\n",
      "  (0, 10)\t0.5068030380839282\n",
      "  (0, 11)\t0.3036893761772086\n",
      "  (0, 12)\t0.06840573860297583\n",
      "  (0, 13)\t0.34927599213080796\n",
      "  (0, 14)\t0.5628071751074061\n",
      "  (0, 15)\t0.452003375648809\n",
      "  (0, 16)\t0.571253537143728\n",
      "  (0, 17)\t0.4246035444312495\n",
      "  (0, 18)\t0.6712020253892854\n",
      "  (0, 19)\t0.2602045571258924\n",
      "  (0, 20)\t0.04100590738541632\n",
      "  (0, 21)\t0.38888888888888873\n",
      "  (0, 22)\t0.6055946811266921\n",
      "  (0, 23)\t0.09850212828958171\n",
      "  (0, 24)\t0.5463035642189196\n",
      "  :\t:\n",
      "  (1469, 1445)\t0.4997835966139753\n",
      "  (1469, 1446)\t0.5454545454545454\n",
      "  (1469, 1447)\t0.4848725736710717\n",
      "  (1469, 1448)\t0.4276361929678575\n",
      "  (1469, 1449)\t0.4197411468143405\n",
      "  (1469, 1450)\t0.4997835966139753\n",
      "  (1469, 1451)\t0.5067799747921895\n",
      "  (1469, 1452)\t0.28454524120982194\n",
      "  (1469, 1453)\t0.6873473002596386\n",
      "  (1469, 1454)\t0.5691797815723354\n",
      "  (1469, 1455)\t0.5320903998212967\n",
      "  (1469, 1456)\t0.4777670321329064\n",
      "  (1469, 1457)\t0.4153154178481695\n",
      "  (1469, 1458)\t0.4276361929678575\n",
      "  (1469, 1459)\t0.4770422115649787\n",
      "  (1469, 1460)\t0.5357929174514724\n",
      "  (1469, 1461)\t0.6376284623302606\n",
      "  (1469, 1462)\t0.5357929174514724\n",
      "  (1469, 1463)\t0.3036893761772086\n",
      "  (1469, 1464)\t0.6228317454293213\n",
      "  (1469, 1465)\t0.6046522536925664\n",
      "  (1469, 1466)\t0.606060606060606\n",
      "  (1469, 1467)\t0.44093460798032524\n",
      "  (1469, 1468)\t0.5123393477776219\n",
      "  (1469, 1469)\t-2.220446049250313e-16\n"
     ]
    }
   ],
   "source": [
    "print(reg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71255ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "GRAMBANK_LANGUAGES = l2v.available_grambank()\n",
    "GRAMBANK_DISTANCES =  \"/Users/CitronVert/Desktop/langrank/gram2vec/lang2vec/data/learned.npy\"\n",
    "with open(GRAMBANK_DISTANCES, \"rb\") as f:\n",
    "    feat_pred = np.load(GRAMBANK_DISTANCES, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc07ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718b3f4c-f4a4-438d-a0b5-5e26964b288c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for every pair of languages that is present in both grambank and lang2vec,\n",
    "#find the cosine difference between their feature vectors\n",
    "import scipy.sparse as sparse\n",
    "\n",
    "res = np.zeros((d,d))\n",
    "for x in range(d):\n",
    "    l1 = intersection[x]\n",
    "    v1 = get_vector(v, l1)\n",
    "    for y in range(x,d):\n",
    "        l2 = intersection[y]\n",
    "        v2 = get_vector(v, l2)\n",
    "        diff = spatial.distance.cosine(v1, v2)\n",
    "        res[x][y] = diff\n",
    "\n",
    "\n",
    "# #saves the distance measures as a sparse matrix (for compatability with lang2vec)\n",
    "sA = sparse.csr_matrix(res)\n",
    "sparse.save_npz(\"./concat_distances.npz\", sA)\n",
    "               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7cc9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "intersection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b22cc3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = intersection.index(\"mij\")\n",
    "y = intersection.index(\"sed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4bbc757",
   "metadata": {},
   "outputs": [],
   "source": [
    "res[y][x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1f1181-537a-4c0d-8c20-cd3da803cdd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#saves a list of the languages for which we have distance values\n",
    "data_string = \",\".join(str(item) for item in intersection)\n",
    "\n",
    "    # Open the file in write mode\n",
    "with open(\"./URIEL_distance_langs.txt\", 'w') as file:\n",
    "    # Write the data string to the file\n",
    "    file.write(data_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96922447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training code for reference\n",
    "# def test_train():\n",
    "#     langs = [\"aze\", \"ben\", \"fin\"]\n",
    "#     datasets = [os.path.join(root, \"sample-data\", \"ted-train.orig.{}\".format(l)) for l in langs]\n",
    "#     seg_datasets = [os.path.join(root, \"sample-data\", \"ted-train.orig.spm8000.{}\".format(l)) for l in langs]\n",
    "#     rank = [[0, 1, 2], [1, 0, 2], [2, 1, 0]] # random\n",
    "#     tmp_dir = \"tmp\"\n",
    "#     prepare_train_file(datasets=datasets, segmented_datasets=seg_datasets,\n",
    "#                        langs=langs, rank=rank, tmp_dir=tmp_dir, task=\"MT\")\n",
    "#     output_model = \"{}/model.txt\".format(tmp_dir)\n",
    "#     train(tmp_dir=tmp_dir, output_model=output_model)\n",
    "#     assert os.path.isfile(output_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a3c340",
   "metadata": {},
   "outputs": [],
   "source": [
    "TED_PATH = \"/Users/CitronVert/Desktop/langrank/indexed/MT/ted.npy\"\n",
    "def create_MT_dataset(data_path, lang): \n",
    "    data = np.load(data_path, encoding='latin1', allow_pickle=True).item()\n",
    "    filename = \"ted-train.orig.\"+lang\n",
    "    \n",
    "\n",
    "\n",
    "def train(langs, datasets, seg_datasets, rank): \n",
    "    tmp_dir = \"tmp\"\n",
    "    prepare_train_file(datasets=datasets, segmented_datasets=seg_datasets,\n",
    "                       langs=langs, rank=rank, tmp_dir=tmp_dir, task=\"MT\")\n",
    "    output_model = \"{}/model.txt\".format(tmp_dir)\n",
    "    train(tmp_dir=tmp_dir, output_model=output_model)\n",
    "    assert os.path.isfile(output_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015e5b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to compute the ndcg score\n",
    "#lang: ISO code for task language\n",
    "#ranked_langs: gold rankings for task\n",
    "#predicted: predicted ranking for lang\n",
    "#gamma_max: hyper param from lin et al.\n",
    "\n",
    "import sklearn.metrics as sm\n",
    "import numpy as np\n",
    "def compute_ndcg(lang, ranked_langs, predicted, gamma_max= 10):\n",
    "    ranking_langs = ranked_langs[lang][0] # list of languages for looking up index in ranking vector\n",
    "    # gives position in ranking based on index (if ranking[0] = 4 then the 0th language [ranking_langs[0]] is the 4th best)\n",
    "    ranking = ranked_langs[lang][1] \n",
    "    # creates vector to look up the relevance score of a given language by index\n",
    "    scores_by_index = [0] * len(ranking)\n",
    "    for i in range(len(ranking)): \n",
    "        if ranking[i] <= gamma_max:\n",
    "            scores_by_index[i] = gamma_max - (ranking[i] - 1)\n",
    "    print(\"scores by index\")\n",
    "    print(scores_by_index)\n",
    "    ideal_score = [i for i in reversed(range(1, gamma_max + 1))] + [0] * (len(ranking) - gamma_max)\n",
    "    print(\"IDEAL\")\n",
    "    print(ideal_score)\n",
    "    predicted_score = [0] * len(ranking)\n",
    "    for j in range(len(predicted)): #for each language in ranking\n",
    "        code = predicted[j]\n",
    "        print(code)\n",
    "        index = ranking_langs.index(code)\n",
    "        print(ranking[index])\n",
    "        score = scores_by_index[index] #finds the true relevance of each language\n",
    "        predicted_score[j] = score\n",
    "    return sm.ndcg_score(np.asarray([ideal_score]), np.asarray([predicted_score]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08ba17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a small ndcg test\n",
    "# remember-- we don't exclude the test language\n",
    "dummy = {\n",
    "    \"aze\": ([\"ben\", \"fin\", \"deu\", \"eng\"],[1, 2, 3, 4]),\n",
    "    \"ben\": ([\"aze\",\"fin\", \"deu\", \"eng\"],[1, 3, 2, 4]),\n",
    "    \"fin\": ([\"aze\", \"ben\", \"deu\", \"eng\"],[3, 4, 2, 1]),\n",
    "    \"deu\": ([\"aze\", \"ben\", \"fin\", \"eng\"],[4, 3, 2, 1]),\n",
    "    \"eng\": ([\"aze\", \"ben\", \"fin\", \"deu\"],[4, 3, 2, 1])\n",
    "}\n",
    "lang = \"eng\"\n",
    "predicted = [\"fin\",\"deu\",\"aze\",\"ben\"]\n",
    "gamma_max = 2 \n",
    "print(compute_ndcg(lang, dummy, predicted, gamma_max))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b494daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify the code to accept params instead of freakin data\n",
    "# pull list of MT languages from ... somewhere\n",
    "# run write a function to run langrank_predict with the all model for all langs in list and return ndcg\n",
    "# average list\n",
    "# is it same??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a68d458",
   "metadata": {},
   "outputs": [],
   "source": [
    "import langrank as lr\n",
    "import pickle\n",
    "with open(\"./MT_ranked_ties.pkl\", 'rb') as f:\n",
    "    rankings = pickle.load(f)\n",
    "languages = list(rankings.keys())\n",
    "\n",
    "predicted = {}\n",
    "for lang in languages:\n",
    "    prepared = lr.prepare_featureset(lang)\n",
    "    predicted[lang] = lr.rank(prepared, task=\"MT\", candidates=\"all\", return_langs = True, uriel = True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff88c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./MT_full_ranked_no_ties.pkl\", 'rb') as f:\n",
    "    rankings = pickle.load(f)\n",
    "languages = list(rankings.keys())\n",
    "rank = rankings[\"aze\"][1]\n",
    "gold = [0]*len(languages)\n",
    "for i in range(len(languages)):\n",
    "    gold[rank[i]-1] = languages[i]\n",
    "print(gold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07f2224",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"./uriel_predictions.pkl\", 'rb') as f:\n",
    "    predictions = pickle.load(f)\n",
    "print(predictions[\"aze\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d235744",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./MT_full_ranked_ties.pkl\", 'rb') as f:\n",
    "    rankings = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b298398",
   "metadata": {},
   "outputs": [],
   "source": [
    "lang = \"aze\"\n",
    "ranking_langs = rankings[lang][0] # list of languages for looking up index in ranking vector\n",
    "ranking = rankings[lang][1] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef59a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "langs = [\"\"] * len(ranking_langs)\n",
    "for i in range(len(ranking_langs)):\n",
    "    langs[ranking[i] - 1] = ranking_langs[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518262ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "langs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053616e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d204875",
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = {\"MT\", \"EL\", \"DEP\", \"POS\"}\n",
    "cases = {\"uriel\", \"gram\"}\n",
    "path = \"/Users/CitronVert/Desktop/langrank/results\"\n",
    "for task in tasks:\n",
    "    for case in cases:\n",
    "        p = \"{base}/{task}/{task}_{case}_predictions.pkl\".format(base = path, task = task, case = case)\n",
    "        with open(p, 'rb') as f:\n",
    "            predictions = pickle.load(f)\n",
    "        n = \"{base}/{task}/{task}_{case}_ndcg.pkl\".format(base = path, task = task, case = case)\n",
    "        with open(n, 'rb') as f:\n",
    "            scores = pickle.load(f)\n",
    "        vals = [i for i in list(scores.values())]\n",
    "        # vals\n",
    "        predict = pd.DataFrame.from_dict(predictions)\n",
    "        scores_d = pd.DataFrame( columns = list(scores.keys()))\n",
    "        scores_d.loc[len(scores_d)]= vals \n",
    "        predict.to_csv(r\"{base}/{task}/{task}_{case}_predictions.csv\".format(base = path, task = task, case = case))\n",
    "        scores_d.to_csv(r\"{base}/{task}/{task}_{case}_ndcg.csv\".format(base = path, task = task, case = case))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effe664d",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_d.loc[len(scores_d)]= vals "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34dfb259",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def percent_missing(lang, path):\n",
    "    v = pd.read_csv(path, sep='\\t')\n",
    "    glotto = iso2glotto[lang]\n",
    "    vector = list(get_vector(v, glotto))\n",
    "    a = sum(math.isnan(x) for x in vector)\n",
    "    return a/len(vector)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a06446e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_task_langs(task):\n",
    "    with open(\"./training-data/{}_ranked_train_no_ties.pkl\".format(task), 'rb') as f:\n",
    "        rankings = pickle.load(f)\n",
    "    languages = list(rankings.keys())\n",
    "    return languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d2a444",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"/Users/CitronVert/Desktop/grambank-grambank-analysed-fcf971a/R_grambank/output/GB_wide/GB_cropped_for_missing.tsv\"\n",
    "tasks = [\"MT\", \"DEP\", \"POS\", \"EL\"]\n",
    "for task in tasks:\n",
    "    d = {}\n",
    "    langs = get_task_langs(task)\n",
    "    for lang in langs: \n",
    "        missing = percent_missing(lang, PATH)\n",
    "        d[lang] = missing\n",
    "    with open(r\"./{task}_percent_missing.txt\".format(task = task), 'w') as fp:\n",
    "        for item in d.keys():\n",
    "            percent = d[item]\n",
    "            # write each item on a new line\n",
    "            fp.write(\"{lang}: {percent}\\n\".format(lang = item, percent = percent))\n",
    "        print('Done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487a95db",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'./uriel_features.txt', 'w') as fp:\n",
    "    for item in features:\n",
    "        # write each item on a new line\n",
    "        fp.write(\"%s\\n\" % item)\n",
    "    print('Done')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "grambank",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
