{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7bfc30f6-470a-402c-b0a8-bab8789cd60d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/projects/enri8153/code-server/tmp/ipykernel_1363360/1901553977.py:3: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import pandas as pd\n",
    "from numpy.linalg import norm\n",
    "from scipy import spatial\n",
    "from zipfile import ZipFile as zf\n",
    "import gram2vec.lang2vec.lang2vec as l2v\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import scipy.stats as ss\n",
    "import sklearn.metrics as sm\n",
    "from statistics import mean\n",
    "import os\n",
    "import pkg_resources\n",
    "# import pyconll\n",
    "from iso639 import Lang\n",
    "import os\n",
    "import fnmatch\n",
    "import langrank as lr\n",
    "import defaults as defaults\n",
    "import re\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd84e4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = np.load(\"/projects/enri8153/langrank/indexed/POS/ud.npy\", encoding='latin1', allow_pickle=True).item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41841d7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['lang', 'dataset_size', 'token_number', 'type_number', 'word_vocab', 'type_token_ratio'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d['datasets/pos/lv_lvtb'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "602a40e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/projects/enri8153/langrank/resources/transfer_langs.txt\") as file:\n",
    "    langs = [line.rstrip() for line in file]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "47a778d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import open\n",
    "from conllu import parse_incr\n",
    "# datasets = {}\n",
    "for lang in langs:\n",
    "    tokens = []\n",
    "    types = set()\n",
    "    lines = 0\n",
    "    data_file = open(f\"/projects/enri8153/langrank/conllu/2000/{lang}_train.conllu\", \"r\", encoding=\"utf-8\")\n",
    "    for tokenlist in parse_incr(data_file):\n",
    "        lines+=1\n",
    "        new_tokens = [token[\"form\"] for token in tokenlist]\n",
    "        tokens.extend(new_tokens)\n",
    "        types.update(tokens)\n",
    "    datasets[f\"{lang}_transfer\"] = {'lang': lang, \n",
    "    'dataset_size': lines,\n",
    "    'token_number': len(tokens),\n",
    "    'type_number': len(types),\n",
    "    'word_vocab': types,\n",
    "    'type_token_ratio': len(types)/len(tokens)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "12b3d85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"/projects/enri8153/langrank/indexed/POS/new_ud.npy\", datasets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4d4c423",
   "metadata": {},
   "outputs": [],
   "source": [
    "def levenshtein_distance(list1, list2):\n",
    "    len1, len2 = len(list1), len(list2)\n",
    "    \n",
    "    # Create a matrix of zeros\n",
    "    dp = [[0] * (len2 + 1) for _ in range(len1 + 1)]\n",
    "    \n",
    "    # Initialize the matrix\n",
    "    for i in range(len1 + 1):\n",
    "        dp[i][0] = i\n",
    "    for j in range(len2 + 1):\n",
    "        dp[0][j] = j\n",
    "    \n",
    "    # Compute the distance\n",
    "    for i in range(1, len1 + 1):\n",
    "        for j in range(1, len2 + 1):\n",
    "            cost = 0 if list1[i - 1] == list2[j - 1] else 1\n",
    "            dp[i][j] = min(\n",
    "                dp[i - 1][j] + 1,    # Deletion\n",
    "                dp[i][j - 1] + 1,    # Insertion\n",
    "                dp[i - 1][j - 1] + cost  # Substitution\n",
    "            )\n",
    "    \n",
    "    return dp[len1][len2]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f66949d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for lang1 in langs:\n",
    "    test_name = f\"/projects/enri8153/langrank/results/importance/single_target/{lang1}_stanza_syntax_grambank.tsv\"\n",
    "    if os.path.isfile(test_name):\n",
    "        test = pd.read_csv(test_name, sep='\\t', header=None, index_col=False)\n",
    "        filtered_test = test[test[1] != 0]\n",
    "        features_1 = list(filtered_test[0])\n",
    "    else:\n",
    "        continue\n",
    "    for lang2 in langs:\n",
    "        compare_name =f\"/projects/enri8153/langrank/results/importance/single_target/{lang2}_stanza_syntax_grambank.tsv\"\n",
    "        if os.path.isfile(compare_name):\n",
    "            compare = pd.read_csv(compare_name, sep='\\t', header=None, index_col=False)\n",
    "            filtered_compare = compare[compare[1] != 0]\n",
    "            assert(filtered_compare.shape != compare.shape)\n",
    "            features_2 = list(filtered_compare[1])\n",
    "            ld = levenshtein_distance(features_1, features_2)\n",
    "            with open(\"/projects/enri8153/langrank/results/importance/single_target/stanza_pairwise_levenshtein.tsv\", \"a\") as f:\n",
    "                f.write(\"\\t\".join([lang1, lang2, str(ld)]) + \"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c7557319",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nPrepares a reference list for looking up rank scores; \\nreturns a list of the same length as ranking but ranks gives a score instead\\nif gamma max were 3 then [0, 5 , 3, 4, 2, 1] would become [3, 0, 0, 0, 1, 2]\\nNOT USING BECAUSE IT DONT MAKE SENSE TO ASSIGN ONE TO THE IDEAL\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#iterate through target language target language pairs\n",
    "#pull up single-target model feature importance ranking files for both members of the pair\n",
    "#compute ncdg between the two files\n",
    "#write to csv lang, lang, score\n",
    "\n",
    "\n",
    "#how to compute ncdg?\n",
    "#two languages: test and compare\n",
    "# make set out of feature column to serve as index lookup, indices\n",
    "#create rankings of the form ranking = [0,2,3,1] where the rank in ranking[i] corresponds to the feature in indices[i] \n",
    "# (for the \"test\" language this should look like [0,1,2,3...])\n",
    "#use the scores_ranking to Prepares a reference list for looking up rank scores; returns a list of the same length as ranking but ranks gives a score instead\n",
    "#if gamma max were 3 then [0, 5 , 3, 4, 2, 1] would become [3, 0, 0, 0, 1, 2]\n",
    "#THEN, we can compare with sm.ndcg_score(np.asarray([golds_scores]), np.asarray([compare_scores]),k=k)\n",
    "\n",
    "'''\n",
    "Prepares a reference list for looking up rank scores; \n",
    "returns a list of the same length as ranking but ranks gives a score instead\n",
    "if gamma max were 3 then [0, 5 , 3, 4, 2, 1] would become [3, 0, 0, 0, 1, 2]\n",
    "NOT USING BECAUSE IT DONT MAKE SENSE TO ASSIGN ONE TO THE IDEAL\n",
    "'''\n",
    "# def scores_ranking(ranking, gamma_max = 10):\n",
    "#     scores_by_index = [0] * len(ranking)\n",
    "#     for i in range(len(ranking)): \n",
    "#         if ranking[i] <= gamma_max:\n",
    "#             scores_by_index[i] = gamma_max - (ranking[i])\n",
    "#     return scores_by_index\n",
    "\n",
    "\n",
    "# for lang1 in langs:\n",
    "#     test_name = f\"/projects/enri8153/langrank/results/importance/single_target/{lang1}_stanza_syntax_grambank.tsv\"\n",
    "#     if os.path.isfile(test_name):\n",
    "#         test = pd.read_csv(test_name, sep='\\t', header=None, index_col=False)\n",
    "#         indices = list(test[0])\n",
    "#         test_ranking = [i for i in range(len(indices))]\n",
    "#         print(lang1)\n",
    "\n",
    "#     else:\n",
    "#         continue\n",
    "#     for lang2 in langs:\n",
    "#         print(lang2)\n",
    "#         compare_name =f\"/projects/enri8153/langrank/results/importance/single_target/{lang2}_stanza_syntax_grambank.tsv\"\n",
    "#         if os.path.isfile(compare_name):\n",
    "#             compare = pd.read_csv(compare_name, sep='\\t', header=None, index_col=False)\n",
    "#             features = list(compare[0])\n",
    "#             compare_ranking = [features.index(i) for i in indices]\n",
    "#             compare_scores = scores_ranking(compare_ranking)\n",
    "#             print(compare_scores)\n",
    "\n",
    "#             test_scores = scores_ranking(test_ranking)\n",
    "#             print(test_scores)\n",
    "#             ncdg = sm.ndcg_score(np.asarray([test_scores]), np.asarray([compare_scores]),k=5)\n",
    "#             with open(\"/projects/enri8153/langrank/results/importance/single_target/stanza_pairwise.tsv\", \"a\") as f:\n",
    "#                 f.write(\"\\t\".join([lang1, lang2, str(ncdg)]) + \"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9cfe9b74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'GB020': 'deixis', 'GB021': 'deixis', 'GB022': 'deixis', 'GB023': 'deixis', 'GB035': 'deixis', 'GB036': 'deixis', 'GB037': 'deixis', 'GB038': 'deixis', 'GB313': 'deixis', 'GB028': 'number', 'GB031': 'number', 'GB039': 'number', 'GB042': 'number', 'GB043': 'number', 'GB044': 'number', 'GB165': 'number', 'GB166': 'number', 'GB184': 'number', 'GB185': 'number', 'GB186': 'number', 'GB316': 'number', 'GB317': 'number', 'GB318': 'number', 'GB319': 'number', 'GB320': 'number', 'GB030': 'class', 'GB051': 'class', 'GB052': 'class', 'GB053': 'class', 'GB054': 'class', 'GB058': 'class', 'GB059': 'class', 'GB170': 'class', 'GB171': 'class', 'GB172': 'class', 'GB192': 'class', 'GB196': 'class', 'GB197': 'class', 'GB198': 'class', 'GB321': 'class', 'GB057': 'quantification', 'GB068': 'non-verbal predication', 'GB069': 'non-verbal predication', 'GB117': 'non-verbal predication', 'GB070': 'argument marking (core)', 'GB071': 'argument marking (core)', 'GB089': 'argument marking (core)', 'GB090': 'argument marking (core)', 'GB091': 'argument marking (core)', 'GB092': 'argument marking (core)', 'GB093': 'argument marking (core)', 'GB094': 'argument marking (core)', 'GB095': 'argument marking (core)', 'GB096': 'argument marking (core)', 'GB098': 'argument marking (core)', 'GB099': 'argument marking (core)', 'GB109': 'argument marking (core)', 'GB149': 'argument marking (core)', 'GB177': 'argument marking (core)', 'GB072': 'argument marking (non-core)', 'GB073': 'argument marking (non-core)', 'GB074': 'argument marking (non-core)', 'GB075': 'argument marking (non-core)', 'GB079': 'argument marking (non-core)', 'GB080': 'argument marking (non-core)', 'GB103': 'argument marking (non-core)', 'GB104': 'argument marking (non-core)', 'GB105': 'argument marking (non-core)', 'GB108': 'argument marking (non-core)', 'GB116': 'argument marking (non-core)', 'GB081': 'VP (other)', 'GB107': 'VP (other)', 'GB111': 'VP (other)', 'GB158': 'VP (other)', 'GB298': 'VP (other)', 'GB299': 'VP (other)', 'GB082': 'TAME', 'GB083': 'TAME', 'GB084': 'TAME', 'GB086': 'TAME', 'GB110': 'TAME', 'GB119': 'TAME', 'GB120': 'TAME', 'GB121': 'TAME', 'GB139': 'TAME', 'GB309': 'TAME', 'GB312': 'TAME', 'GB113': 'valency', 'GB114': 'valency', 'GB115': 'valency', 'GB147': 'valency', 'GB155': 'valency', 'GB129': 'verb complex', 'GB131': 'clause order', 'GB132': 'clause order', 'GB133': 'clause order', 'GB136': 'clause order', 'GB137': 'clause order', 'GB138': 'clause order', 'GB130': 'clause order', 'GB430': 'order in NP', 'GB431': 'order in NP', 'GB432': 'order in NP', 'GB433': 'order in NP', 'GB024': 'order in NP', 'GB025': 'order in NP', 'GB065': 'order in NP', 'GB193': 'order in NP'}\n",
      "\n",
      "Source: xlmr\n",
      "deixis: GB022\n",
      "VP (other): GB298\n",
      "argument marking (core): GB098\n",
      "argument marking (non-core): GB079\n",
      "order in NP: GB432\n",
      "VP (other): GB111\n",
      "argument marking (core): GB093\n",
      "other: GB193a\n",
      "class: GB053\n",
      "other: GB024b\n",
      "deixis: 0.1\n",
      "VP (other): 0.2\n",
      "argument marking (core): 0.2\n",
      "argument marking (non-core): 0.1\n",
      "order in NP: 0.1\n",
      "other: 0.2\n",
      "class: 0.1\n",
      "\n",
      "Source: stanza\n",
      "class: GB172\n",
      "argument marking (non-core): GB073\n",
      "argument marking (non-core): GB080\n",
      "VP (other): GB299\n",
      "clause order: GB137\n",
      "other: GB193a\n",
      "deixis: GB020\n",
      "deixis: GB022\n",
      "class: GB059\n",
      "clause order: GB132\n",
      "class: 0.2\n",
      "argument marking (non-core): 0.2\n",
      "VP (other): 0.1\n",
      "clause order: 0.2\n",
      "other: 0.1\n",
      "deixis: 0.2\n"
     ]
    }
   ],
   "source": [
    "def read_ablations_dictionary(path = defaults.ABLATIONS_PATH):\n",
    "    key = \"Grambank\"\n",
    "    a = pd.read_csv(open(path, 'rb'))\n",
    "    categories = list(a[\"Categories\"])[:-2]\n",
    "    cat_dict = {}\n",
    "    for cat in categories: #what to exclude?\n",
    "        # if cat in [\"quantification\", \"non-verbal predication\", \"argument marking (non-core)\", \"valency\", \"verb complex\"]: \n",
    "        #         continue\n",
    "        category_rows = a[a[\"Categories\"] == cat]\n",
    "    \n",
    "        # Check if there's at least one row for the category\n",
    "        if not category_rows.empty:\n",
    "            feats = category_rows.iloc[0][key].split(\",\")\n",
    "            pattern = r'[^\\w\\s]|_'\n",
    "            # Use re.sub() to replace all matches of the pattern with an empty string\n",
    "            feats  = [re.sub(pattern, '', s) for s in feats] \n",
    "            for feat in feats:\n",
    "                cat_dict[feat.strip()] = cat\n",
    "            # Print the features\n",
    "        else:\n",
    "            print(f\"No rows found for category '{cat}'\")\n",
    "    return cat_dict\n",
    "\n",
    "\n",
    "K = 10\n",
    "cat_dict = read_ablations_dictionary()\n",
    "print(cat_dict)\n",
    "cat_dict[\"genetic\"] = \"genetic\"\n",
    "cat_dict[\"featural\"] = \"featural\"\n",
    "cat_dict[\"inventory\"] = \"inventory\"\n",
    "cat_dict[\"phylogenetic\"] = \"phylogenetic\"\n",
    "cat_dict[\"phonological\"] = \"phonological\"\n",
    "\n",
    "for source in [\"xlmr\", \"stanza\"]:\n",
    "    print(f\"\\nSource: {source}\")\n",
    "    # feat_path = f\"/projects/enri8153/langrank/results/for_analysis/average_{source}_gain_syntax_grambank.tsv\"\n",
    "    feat_path = f\"/projects/enri8153/langrank/results/importance/only_grambank/average_{source}_gain_syntax_grambank.tsv\"\n",
    "    df = pd.read_csv(feat_path, sep='\\t', header=None, index_col=False)\n",
    "    feats = list(df[0])[:K]\n",
    "    count_dict = {}\n",
    "    for feat in feats:\n",
    "        if feat in cat_dict:\n",
    "            cat = cat_dict[feat]\n",
    "        else:\n",
    "            cat = \"other\"\n",
    "        print(f\"{cat}: {feat}\")\n",
    "        if cat not in count_dict:\n",
    "            count_dict[cat] = 1\n",
    "        else:\n",
    "            count_dict[cat] = count_dict[cat]+1\n",
    "    for cat in count_dict.keys():\n",
    "        percent = count_dict[cat]/K\n",
    "        print(f\"{cat}: {percent}\")\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d55d5eb",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No objects to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m         info[lang] \u001b[39m=\u001b[39m df\n\u001b[1;32m     15\u001b[0m dfs \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(info\u001b[39m.\u001b[39mvalues())\n\u001b[0;32m---> 16\u001b[0m concat_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mconcat(dfs, keys\u001b[39m=\u001b[39;49m\u001b[39mrange\u001b[39;49m(\u001b[39mlen\u001b[39;49m(dfs)), names\u001b[39m=\u001b[39;49m[\u001b[39m'\u001b[39;49m\u001b[39mDF_Index\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[1;32m     18\u001b[0m \u001b[39m# Reset index to make 'Feature' and 'DF_Index' columns accessible\u001b[39;00m\n\u001b[1;32m     19\u001b[0m concat_df \u001b[39m=\u001b[39m concat_df\u001b[39m.\u001b[39mreset_index(level\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mDF_Index\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m/projects/enri8153/software/anaconda/envs/bankrank/lib/python3.10/site-packages/pandas/core/reshape/concat.py:380\u001b[0m, in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[1;32m    377\u001b[0m \u001b[39melif\u001b[39;00m copy \u001b[39mand\u001b[39;00m using_copy_on_write():\n\u001b[1;32m    378\u001b[0m     copy \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m--> 380\u001b[0m op \u001b[39m=\u001b[39m _Concatenator(\n\u001b[1;32m    381\u001b[0m     objs,\n\u001b[1;32m    382\u001b[0m     axis\u001b[39m=\u001b[39;49maxis,\n\u001b[1;32m    383\u001b[0m     ignore_index\u001b[39m=\u001b[39;49mignore_index,\n\u001b[1;32m    384\u001b[0m     join\u001b[39m=\u001b[39;49mjoin,\n\u001b[1;32m    385\u001b[0m     keys\u001b[39m=\u001b[39;49mkeys,\n\u001b[1;32m    386\u001b[0m     levels\u001b[39m=\u001b[39;49mlevels,\n\u001b[1;32m    387\u001b[0m     names\u001b[39m=\u001b[39;49mnames,\n\u001b[1;32m    388\u001b[0m     verify_integrity\u001b[39m=\u001b[39;49mverify_integrity,\n\u001b[1;32m    389\u001b[0m     copy\u001b[39m=\u001b[39;49mcopy,\n\u001b[1;32m    390\u001b[0m     sort\u001b[39m=\u001b[39;49msort,\n\u001b[1;32m    391\u001b[0m )\n\u001b[1;32m    393\u001b[0m \u001b[39mreturn\u001b[39;00m op\u001b[39m.\u001b[39mget_result()\n",
      "File \u001b[0;32m/projects/enri8153/software/anaconda/envs/bankrank/lib/python3.10/site-packages/pandas/core/reshape/concat.py:443\u001b[0m, in \u001b[0;36m_Concatenator.__init__\u001b[0;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[1;32m    440\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverify_integrity \u001b[39m=\u001b[39m verify_integrity\n\u001b[1;32m    441\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcopy \u001b[39m=\u001b[39m copy\n\u001b[0;32m--> 443\u001b[0m objs, keys \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_clean_keys_and_objs(objs, keys)\n\u001b[1;32m    445\u001b[0m \u001b[39m# figure out what our result ndim is going to be\u001b[39;00m\n\u001b[1;32m    446\u001b[0m ndims \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_ndims(objs)\n",
      "File \u001b[0;32m/projects/enri8153/software/anaconda/envs/bankrank/lib/python3.10/site-packages/pandas/core/reshape/concat.py:505\u001b[0m, in \u001b[0;36m_Concatenator._clean_keys_and_objs\u001b[0;34m(self, objs, keys)\u001b[0m\n\u001b[1;32m    502\u001b[0m     objs_list \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(objs)\n\u001b[1;32m    504\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(objs_list) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 505\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mNo objects to concatenate\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    507\u001b[0m \u001b[39mif\u001b[39;00m keys \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    508\u001b[0m     objs_list \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(com\u001b[39m.\u001b[39mnot_none(\u001b[39m*\u001b[39mobjs_list))\n",
      "\u001b[0;31mValueError\u001b[0m: No objects to concatenate"
     ]
    }
   ],
   "source": [
    "info ={}\n",
    "for arch in [\"orig\", \"xlmr\", \"stanza\"]:\n",
    "    for source in [\"syntax_grambank\", \"syntax_knn\"]:\n",
    "        for dataset_key in [\"dataset\", \"no-dataset\"]:\n",
    "            for distance_key in [\"distance\", \"no-distance\"]:\n",
    "                for lang in langs:\n",
    "                    fname = f\"/projects/enri8153/langrank/results/importance/{dataset_key}/{distance_key}/{lang}_{arch}_{source}.tsv\"\n",
    "                    if os.path.isfile(fname):\n",
    "                        df = pd.read_csv(fname, sep='\\t', header=None, index_col=False)\n",
    "                        df = df.sort_values(0)\n",
    "                        df.columns = [\"Feature\", \"Score\", \"Split\"]\n",
    "                        df.drop(\"Split\", axis=1)\n",
    "                        info[lang] = df\n",
    "\n",
    "                dfs = list(info.values())\n",
    "                concat_df = pd.concat(dfs, keys=range(len(dfs)), names=['DF_Index'])\n",
    "\n",
    "                # Reset index to make 'Feature' and 'DF_Index' columns accessible\n",
    "                concat_df = concat_df.reset_index(level='DF_Index')\n",
    "\n",
    "                # Pivot the concatenated DataFrame to reshape it for averaging\n",
    "                pivot_df = concat_df.pivot_table(index='Feature', columns='DF_Index', values='Score')\n",
    "\n",
    "                # Calculate the average score across the DataFrames\n",
    "                pivot_df['Average Gain'] = pivot_df.mean(axis=1)\n",
    "\n",
    "                # Create the final DataFrame with feature names and averaged scores\n",
    "                result_df = pivot_df[['Average Gain']].reset_index()\n",
    "                result = result_df.sort_values(\"Average Gain\", ascending=False)\n",
    "                # Display the resulting DataFrame\n",
    "                result.to_csv(f\"/projects/enri8153/langrank/results/importance/{dataset_key}/{distance_key}/average_{arch}_gain_{source}.tsv\", sep='\\t', index=False, header=False)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3676ddc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_database = np.load(\"/projects/enri8153/langrank/gram2vec/lang2vec/data/grambank.npz\", allow_pickle = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74d46254",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['GB020', 'GB021', 'GB022', 'GB023', 'GB028', 'GB030', 'GB031',\n",
       "       'GB035', 'GB036', 'GB037', 'GB038', 'GB039', 'GB042', 'GB043',\n",
       "       'GB044', 'GB051', 'GB052', 'GB053', 'GB054', 'GB057', 'GB058',\n",
       "       'GB059', 'GB068', 'GB069', 'GB070', 'GB071', 'GB072', 'GB073',\n",
       "       'GB074', 'GB075', 'GB079', 'GB080', 'GB081', 'GB082', 'GB083',\n",
       "       'GB084', 'GB086', 'GB089', 'GB090', 'GB091', 'GB092', 'GB093',\n",
       "       'GB094', 'GB095', 'GB096', 'GB098', 'GB099', 'GB103', 'GB104',\n",
       "       'GB105', 'GB107', 'GB108', 'GB109', 'GB110', 'GB111', 'GB113',\n",
       "       'GB114', 'GB115', 'GB116', 'GB117', 'GB119', 'GB120', 'GB121',\n",
       "       'GB129', 'GB131', 'GB132', 'GB133', 'GB136', 'GB137', 'GB138',\n",
       "       'GB139', 'GB147', 'GB149', 'GB155', 'GB158', 'GB165', 'GB166',\n",
       "       'GB170', 'GB171', 'GB172', 'GB177', 'GB184', 'GB185', 'GB186',\n",
       "       'GB192', 'GB196', 'GB197', 'GB198', 'GB298', 'GB299', 'GB309',\n",
       "       'GB312', 'GB313', 'GB316', 'GB317', 'GB318', 'GB319', 'GB320',\n",
       "       'GB321', 'GB430', 'GB431', 'GB432', 'GB433', 'GB024a', 'GB024b',\n",
       "       'GB025a', 'GB025b', 'GB065a', 'GB065b', 'GB130a', 'GB130b',\n",
       "       'GB193a', 'GB193b'], dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_database[\"feats\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d23d72f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_language_file = \"/Users/CitronVert/Desktop/grambank-grambank-analysed-fcf971a/grambank/cldf/languages.csv\"\n",
    "langs = pd.read_csv(gb_language_file)\n",
    "lang_names = list(langs[\"Name\"])\n",
    "gram_codes = list(langs[\"ID\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9120d844",
   "metadata": {},
   "outputs": [],
   "source": [
    "gram2iso = {}\n",
    "for i in range(len(lang_names)):\n",
    "    name = lang_names[i]\n",
    "    id = gram_codes[i]\n",
    "    try:\n",
    "        code = Lang(name).pt3\n",
    "        gram2iso[id] = code\n",
    "    except:\n",
    "        print(f\"couldn't find {name}\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2db425fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"gram2iso_dict.pkl\", 'wb') as file:\n",
    "    pickle.dump(gram2iso, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d9fc4846",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_folder_with_string(parent_directory, string):\n",
    "    folders = []\n",
    "    for root, dirs, _ in os.walk(parent_directory):\n",
    "        for dir_name in dirs:\n",
    "            if fnmatch.fnmatch(dir_name, f'*{string}*'):\n",
    "                folders.append(os.path.join(root, dir_name))\n",
    "    return folders\n",
    "def find_files_with_string(parent_directory, string):\n",
    "    files = []\n",
    "    for root, _, filenames in os.walk(parent_directory):\n",
    "        for filename in filenames:\n",
    "            if fnmatch.fnmatch(filename, f'*{string}*'):\n",
    "                files.append(os.path.join(root, filename))\n",
    "    return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8cf9e358",
   "metadata": {},
   "outputs": [],
   "source": [
    "PARENT_DIR = \"/Users/CitronVert/Desktop/ud/ud-treebanks-v2.2\"\n",
    "#search through the local repo for UD-language name-something and select the first one (for now)\n",
    "lang2path ={}\n",
    "for lang in lang_names:\n",
    "    lang_dirs = find_folder_with_string(PARENT_DIR, lang)\n",
    "    if lang_dirs == []: \n",
    "        continue\n",
    "    else:\n",
    "        for dir in lang_dirs:\n",
    "            train = find_files_with_string(dir, \"train.conllu\")\n",
    "            if not train == []:\n",
    "                dev = find_files_with_string(dir, \"dev.conllu\")\n",
    "                test = find_files_with_string(dir, \"test.conllu\")\n",
    "                break\n",
    "    if not train == [] and not dev == [] and not test == []:\n",
    "        lang2path[lang] = [train[0], dev[0], test[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d99bad82",
   "metadata": {},
   "outputs": [],
   "source": [
    "for language in lang2path.keys():\n",
    "    train_path = lang2path[language][0]\n",
    "    train = pyconll.load_from_file(train_path)\n",
    "    train = train[0:500]\n",
    "    path = f\"./conllu/{Lang(language).pt3}_train.conllu\"\n",
    "    with open(path, 'w', encoding='utf-8') as f:\n",
    "        train.write(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "12e58e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/CitronVert/Desktop/langrank/resources/pos_languages.txt\", 'w') as file:\n",
    "    isos = [Lang(language).pt3 for language in lang2path.keys()]\n",
    "    file.writelines('\\n'.join(str(i) for i in isos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "97368a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "for language in lang2path.keys():\n",
    "    test_path = lang2path[language][2]\n",
    "    dev_path = lang2path[language][1]\n",
    "    test = pyconll.load_from_file(test_path)\n",
    "    dev = pyconll.load_from_file(dev_path)\n",
    "    test_path = f\"./conllu/{Lang(language).pt3}_test.conllu\"\n",
    "    dev_path = f\"./conllu/{Lang(language).pt3}_dev.conllu\"\n",
    "    with open(test_path, 'w', encoding='utf-8') as f:\n",
    "        test.write(f)\n",
    "    with open(dev_path, 'w', encoding='utf-8') as f:\n",
    "        dev.write(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "823174d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "converts all lang codes to 3 letter iso codes\n",
    "'''\n",
    "def to_iso(language):\n",
    "    return Lang(language).pt3\n",
    "\n",
    "''' \n",
    "processes gold ranking files to convert all 2 letter iso codes to 3 letters\n",
    "expects \"train/target\" to be in (0,0)\n",
    "'''\n",
    "def convert_datafile_isos(datafile, savepath):\n",
    "\n",
    "    df = pd.read_csv(datafile)\n",
    "    new_columns = {}\n",
    "    for column in df.columns:\n",
    "        if len(column) == 2:  \n",
    "            new_columns[column] = to_iso(column)\n",
    "        \n",
    "    df = df.rename(columns=new_columns)\n",
    "    for index, row in df.iterrows():\n",
    "        if len(row['train/target']) == 2:\n",
    "            df.at[index, 'train/target'] = to_iso(row['train/target'])\n",
    "\n",
    "\n",
    "    df.to_csv(savepath, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eefedc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "takes a list of 3 letter iso codes and removes all 3 letter isocodes from ranking data that are not present in the list\n",
    "'''\n",
    "def match_langs(match, data, column_name = \"train/target\"):\n",
    "    filtered_data = data[data[column_name].isin(match)] #removes train language data not present in list\n",
    "    filtered_data = filtered_data[[column_name] + [col for col in filtered_data.columns[1:] if col in match]] #removes target language data not present in list\n",
    "    return filtered_data\n",
    "\n",
    "def align_and_remove_unique_columns(df1, df2):\n",
    "    # Find columns unique to each dataframe\n",
    "    unique_cols_df1 = set(df1.columns) - set(df2.columns)\n",
    "    unique_cols_df2 = set(df2.columns) - set(df1.columns)\n",
    "    \n",
    "    # Drop unique columns from each dataframe\n",
    "    df1 = df1.drop(columns=unique_cols_df1)\n",
    "    df2 = df2.drop(columns=unique_cols_df2)\n",
    "\n",
    "    # Get the column order of df1\n",
    "    column_order = df1.columns.tolist()\n",
    "    \n",
    "    # Reorder columns of df2 to match df1\n",
    "    df2 = df2[column_order]\n",
    "\n",
    "    return df1, df2\n",
    "\n",
    "def align_and_remove_unique_rows(df1, df2, column_name=\"train/target\"):\n",
    "    # Find values unique to each dataframe in the specified column\n",
    "    unique_values_df1 = set(df1[column_name]) - set(df2[column_name])\n",
    "    unique_values_df2 = set(df2[column_name]) - set(df1[column_name])\n",
    "    \n",
    "    # Drop rows with unique values in the specified column from each dataframe\n",
    "    df1 = df1[~df1[column_name].isin(unique_values_df1)]\n",
    "    df2 = df2[~df2[column_name].isin(unique_values_df2)]\n",
    "     # Use the values in the first column as the index\n",
    "    df1_indexed = df1.set_index('train/target')\n",
    "    df2_indexed = df2.set_index('train/target')\n",
    "    \n",
    "    # Reindex df2 to match the order of df1's index\n",
    "    df2 = df2_indexed.reindex(df1_indexed.index)\n",
    "    \n",
    "    return df1, df2.reset_index()\n",
    "\n",
    "'''\n",
    "Given two organized datasets (all iso codes of 3 letters with \"train/test\" in (0,0)) \n",
    "reindex to match row and column order and save to file\n",
    "'''\n",
    "def match_mtt_xpos(path1, path2, data_dir, fname1=\"aligned_mtt.csv\", fname2=\"aligned_xpos2\"):\n",
    "    xtt_path = data_dir + path1\n",
    "    xpos_path = data_dir + path2\n",
    "    xtt = pd.read_csv(xtt_path)\n",
    "    xpos = pd.read_csv(xpos_path)\n",
    "    xtt, xpos = align_and_remove_unique_columns(xtt, xpos)\n",
    "    xtt, xpos = align_and_remove_unique_rows(xtt, xpos)\n",
    "    xtt.to_csv(data_dir + fname1, index = False)\n",
    "    xpos.to_csv(data_dir + fname2, index = False)\n",
    "\n",
    "'''\n",
    "For whatever reason, langrank includes languages in the raw data that are not actually present in their POS datasets. \n",
    "This removes them so they don't cause issues during the creation of training data\n",
    "'''\n",
    "def remove_no_data_langs(data, task = \"POS\", column_name = \"train/target\"):\n",
    "    target_langs =  list(data.columns)[1:]\n",
    "    train_langs = list(data[column_name])\n",
    "    datasets_dict = lr.map_task_to_data(task)\n",
    "    for dt in datasets_dict:\n",
    "        fn = pkg_resources.resource_filename(__name__, os.path.join('indexed', task, datasets_dict[dt]))\n",
    "        features = np.load(fn, encoding='latin1', allow_pickle=True).item()\n",
    "    missing = set()\n",
    "    languages = set(target_langs + train_langs)\n",
    "    for lang in languages:\n",
    "        code = (lr.PREFIXES[task] + lang)\n",
    "        if not code in features:\n",
    "            code = lr.PREFIXES[task] + Lang(lang).pt1\n",
    "        if task == \"POS\":\n",
    "            code = [a for a in list(features.keys()) if a.startswith(lr.PREFIXES[\"POS\"] + Lang(lang).pt1)]\n",
    "            if len(code) > 0:\n",
    "                code = code[0]\n",
    "        if (code == []) or (not code in features): \n",
    "          missing.add(lang)\n",
    "    data = data[~data[column_name].isin(missing)]\n",
    "    missing_cols = missing.intersection(set(target_langs))\n",
    "    data = data.drop(columns=missing_cols)\n",
    "    return data\n",
    "\n",
    "\n",
    "'''\n",
    "Given a language and an organized dataset, returns \n",
    "ranked: a list of integers where the value at index i is the ranking of the corresponding language (based on order in the dataset) at index i \n",
    "'''\n",
    "def numerical_rank(language, data):\n",
    "    values = list(data[language])\n",
    "    values = [-float(a) for a in values]\n",
    "    ranked =  ss.rankdata(values, method = \"ordinal\") \n",
    "    ranked = [i - 1 for i in ranked]\n",
    "    return ranked\n",
    "\n",
    "'''\n",
    "Given an organized dataset, returns \n",
    "langs_ranked: a dictionary where \n",
    "keys = language and \n",
    "values = a tuple of indices, ranked where the ranking at index i of ranked corresponds to the language at index i of indices\n",
    "'''\n",
    "def make_golds(data, col_name = \"train/target\"):\n",
    "    test = list(data.columns)[1:]\n",
    "    langs_ranked = {}\n",
    "    for language in test: \n",
    "        df = data.copy()\n",
    "        df = df.drop(data.loc[data[col_name].isin([language])].index) #for fairness, remove target language from potential training lanugages\n",
    "        indices = list(df[col_name])\n",
    "        ranked = numerical_rank(language, df)\n",
    "        langs_ranked[language] = (indices, ranked)\n",
    "    return langs_ranked\n",
    "\n",
    "'''\n",
    "Given an organized dataset and a language, returns \n",
    "a tuple of (indices, rankings)\n",
    "indices: a list of ranked languages (full set of test languages - leave one out language)\n",
    "rankings: a dictionary (as in make_golds); full set of test languages ranked with current lang removed\n",
    "'''\n",
    "def make_ranked(data, target_lang, col_name = \"train/target\"):\n",
    "    remove = [target_lang]\n",
    "    # removes current language from ranking (for leave one out)\n",
    "    data = data.drop(remove,axis = 1) \n",
    "    data = data.drop(data.loc[data[col_name].isin(remove)].index) \n",
    "    test = list(data.columns)[1:]\n",
    "    indices = list(data[col_name])\n",
    "    rankings = {}\n",
    "    for test_language in test:\n",
    "        ranked = numerical_rank(test_language, data)\n",
    "        rankings[test_language] = ranked\n",
    "    return (indices, rankings)\n",
    "\n",
    "'''\n",
    "Given a language, task and an organized dataset, saves gold rankings and leave-one-out ranked training data to pickle \n",
    "'''\n",
    "def make_data_pickles(data_path, data_dir, task, training_dir = \"./training-data/\"):\n",
    "    data = pd.read_csv(data_dir + data_path)\n",
    "    data = remove_no_data_langs(data)\n",
    "    golds = make_golds(data)\n",
    "    languages = list(data.columns)[1:]\n",
    "    ranks = {language: make_ranked(data.copy(), language) for language in languages}\n",
    "    save_path = f\"{training_dir}{task}_POS_\"\n",
    "    with open(save_path + \"golds.pkl\", 'wb') as f:\n",
    "        pickle.dump(golds, f)\n",
    "    with open(save_path + \"ranked.pkl\", 'wb') as f:\n",
    "        pickle.dump(ranks, f)\n",
    "\n",
    "'''\n",
    "Prepares a reference list for looking up rank scores; \n",
    "returns a list of the same length as ranking but ranks gives a score instead\n",
    "if gamma max were 3 then [0, 5 , 3, 4, 2, 1] would become [3, 0, 0, 0, 1, 2]\n",
    "'''\n",
    "def scores_ranking(ranking, gamma_max = 10):\n",
    "    scores_by_index = [0] * len(ranking)\n",
    "    for i in range(len(ranking)): \n",
    "        if ranking[i] <= gamma_max:\n",
    "            scores_by_index[i] = gamma_max - (ranking[i])\n",
    "    return scores_by_index\n",
    "\n",
    "\n",
    "# def compute_ndcg(lang, ranked_langs, predicted, gamma_max= 9, k=3):\n",
    "#     ranking_langs = ranked_langs[lang][0] # list of languages for looking up index in ranking vector\n",
    "#     # gives position in ranking based on index (if ranking[0] = 4 then the 0th language [ranking_langs[0]] is the 5th best)\n",
    "#     ranking = ranked_langs[lang][1] \n",
    "#     print(ranking)\n",
    "#     # creates vector to look up the relevance score of a given language by index\n",
    "#     scores_by_index = [0] * len(ranking)\n",
    "#     for i in range(len(ranking)): \n",
    "#         if ranking[i] <= gamma_max:\n",
    "#             scores_by_index[i] = gamma_max - (ranking[i] -1)\n",
    "#     ideal_score = [i for i in reversed(range(1, gamma_max + 1))] + [0] * (len(ranking) - gamma_max)\n",
    "#     print(ideal_score)\n",
    "#     predicted_score = [0] * len(ranking)\n",
    "#     for j in range(len(predicted)): #for each language in ranking\n",
    "#         code = predicted[j]\n",
    "#         index = ranking_langs.index(code)\n",
    "#         score = scores_by_index[index] #finds the true relevance of each language\n",
    "#         predicted_score[j] = score\n",
    "#     print(predicted_score)\n",
    "#     return sm.ndcg_score(np.asarray([ideal_score]), np.asarray([predicted_score]),k=k)\n",
    "\n",
    "\n",
    "# >>> # we have groud-truth relevance of some answers to a query:\n",
    "# >>> true_relevance = np.asarray([[10, 0, 0, 1, 5]])\n",
    "# >>> # we predict some scores (relevance) for the answers\n",
    "# >>> scores = np.asarray([[.1, .2, .3, 4, 70]])\n",
    "# >>> ndcg_score(true_relevance, scores)\n",
    "# 0.69...\n",
    "'''\n",
    "Computes ncdg for a single pair of rankings. ranking and compare_ranking MUST be indexed the same\n",
    "'''\n",
    "def compute_ncdg(indices, ranking, compare_ranking, gamma_max = 10 , k = 3):\n",
    "    # gives position in ranking based on index (if ranking[0] = 4 then the 0th language [ranking_langs[0]] is the 5th best)\n",
    "    ranking = ranking \n",
    "    # creates vector to look up the relevance score of a given language by index\n",
    "    golds_scores = scores_ranking(ranking, gamma_max)\n",
    "    compare_scores = scores_ranking(compare_ranking, gamma_max)\n",
    "    # ideal_score = [i for i in reversed(range(1, gamma_max + 1))] #creates an ideal score list up to gamma-max \n",
    "    # scores =  []\n",
    "    # for i in range(gamma_max, 0, -1): \n",
    "    #     a = compare_scores.index(i) #gets the index of the predicted top n languages\n",
    "    #     scores.append(golds_scores[a]) #appends the actual relevance score of each predicted language (based on gold rankings)\n",
    "    return sm.ndcg_score(np.asarray([golds_scores]), np.asarray([compare_scores]),k=k)\n",
    "\n",
    "\n",
    "'''\n",
    "Given two gold data pickles; compute the average ncdg@3 (for comparing xpos and mtt)\n",
    "'''\n",
    "def compute_ncdg_from_golds(data_path, compare_path, training_dir= './training-data/'):\n",
    "    golds = pickle.load(open(f'{training_dir}{data_path}', 'rb'))\n",
    "    compare = pickle.load(open(f'{training_dir}{compare_path}', 'rb'))\n",
    "    assert list(golds.keys()) == list(compare.keys())\n",
    "    scores = []\n",
    "    for lang in golds.keys():\n",
    "        gold_indices = golds[lang][0]\n",
    "        compare_indices = compare[lang][0]\n",
    "        assert gold_indices == compare_indices\n",
    "        gold_ranking = golds[lang][1]\n",
    "        compare_ranking = compare[lang][1]\n",
    "        assert len(compare_ranking) == len(gold_indices)\n",
    "        scores.append(compute_ncdg(gold_indices,gold_ranking, compare_ranking))\n",
    "    return str(mean(scores))\n",
    "    \n",
    "'''\n",
    "Given a pandas dataframe of sorted data, returns a dictionary of the top 3 languages \n",
    "'''\n",
    "def get_topk_training_languages(data , k=3):\n",
    "    top_train_languages = {}\n",
    "    df = data.copy()\n",
    "    # Iterate over each target language column\n",
    "    for target_language in df.columns[1:]:\n",
    "        # Sort the dataframe by the values of the current target language column\n",
    "        sorted_df = df.sort_values(by=target_language, ascending=False)\n",
    "\n",
    "        # Get the top 3 train languages for the current target language\n",
    "        top_train = sorted_df['train/target'].head(k).tolist()\n",
    "\n",
    "        # Store the top 3 train languages in the dictionary\n",
    "        top_train_languages[target_language] = top_train\n",
    "\n",
    "    return top_train_languages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dcf0a34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_datafile_isos(\"./golds/udpipe_scores.csv\", \"./golds/mtt_scores_cleaned.csv\")\n",
    "match_mtt_xpos(\"xpos_scores_cleaned.csv\",\"mtt_scores_cleaned.csv\", \"./golds/\", fname1=\"aligned_xpos.csv\",fname2= \"aligned_mtt.csv\")\n",
    "make_data_pickles(\"mtt_scores_cleaned.csv\", \"./golds/\", \"mtt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9dec4469",
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_datafile_isos(\"./golds/xlmr_500_scores.csv\", \"./golds/xpos_scores_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8b97c946",
   "metadata": {},
   "outputs": [],
   "source": [
    "xpos = pd.read_csv(\"/Users/CitronVert/Desktop/langrank/golds/xpos_scores_cleaned.csv\")\n",
    "train = list(xpos[\"train/target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b4a0e58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "gram = l2v.available_distance_languages()\n",
    "for lang in train:\n",
    "    if not lang in gram:\n",
    "        print(lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8974e126",
   "metadata": {},
   "outputs": [],
   "source": [
    "xpos = pd.read_csv(\"/Users/CitronVert/Desktop/langrank/golds/xpos_scores_cleaned.csv\")\n",
    "train = list(xpos[\"train/target\"])\n",
    "xpos = match_langs(train, xpos)\n",
    "with open(\"/Users/CitronVert/Desktop/langrank/golds/xpos_scores_cleaned.csv\", 'w') as f:\n",
    "    xpos.to_csv(f, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9da6d0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "match_mtt_xpos(\"xtt_pos_scores_clean.csv\", \"xpos_scores_cleaned.csv\", \"/Users/CitronVert/Desktop/langrank/golds/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da537f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_data_pickles(\"xpos_scores_cleaned.csv\", \"./golds/\", \"xpos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5836c434",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_data_pickles(\"aligned_xtt.csv\", \"./golds/\", \"mtt\")\n",
    "make_data_pickles(\"aligned_xpos.csv\", \"./golds/\", \"xpos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b9f8e1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53\n",
      "53\n"
     ]
    }
   ],
   "source": [
    "train = pickle.load(open(\"/projects/enri8153/langrank/training-data/mtt_POS_ranked.pkl\", 'rb'))\n",
    "gold =  pickle.load(open(\"/projects/enri8153/langrank/training-data/mtt_POS_golds.pkl\", 'rb'))\n",
    "print(len(train[\"pcm\"][0]))\n",
    "print(len(gold[\"pcm\"][0]))\n",
    "# b = pickle.load(open(\"/projects/enri8153/langrank/training-data/xpos_POS_ranked.pkl\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b05a59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "truth = [len(a[lang][0]) == len(b[lang][0]) for lang in a.keys()]\n",
    "assert False not in truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "5371522e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.41797699874321753'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_ncdg_from_golds(\"xtt_golds.pkl\", \"xpos_golds.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3f8c0d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "xpos = pickle.load(open('./training-data/xpos_golds.pkl', 'rb'))\n",
    "xtt =  pickle.load(open('./training-data/xtt_golds.pkl', 'rb'))\n",
    "assert(list(xpos.keys()) == list(xtt.keys()))\n",
    "assert xtt[\"pcm\"][1].index(0) == xtt[\"pcm\"][0].index(\"eng\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7e836395",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './training-data/xpos_golds.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m xpos \u001b[39m=\u001b[39m pickle\u001b[39m.\u001b[39mload(\u001b[39mopen\u001b[39;49m(\u001b[39m'\u001b[39;49m\u001b[39m./training-data/xpos_golds.pkl\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m'\u001b[39;49m))\n\u001b[1;32m      2\u001b[0m xtt \u001b[39m=\u001b[39m  pickle\u001b[39m.\u001b[39mload(\u001b[39mopen\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m./training-data/xtt_golds.pkl\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[1;32m      3\u001b[0m \u001b[39massert\u001b[39;00m(\u001b[39mlist\u001b[39m(xpos\u001b[39m.\u001b[39mkeys()) \u001b[39m==\u001b[39m \u001b[39mlist\u001b[39m(xtt\u001b[39m.\u001b[39mkeys()))\n",
      "File \u001b[0;32m/projects/enri8153/software/anaconda/envs/bankrank/lib/python3.10/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[39mif\u001b[39;00m file \u001b[39min\u001b[39;00m {\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIPython won\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt let you open fd=\u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m by default \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39myou can use builtins\u001b[39m\u001b[39m'\u001b[39m\u001b[39m open.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[39mreturn\u001b[39;00m io_open(file, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './training-data/xpos_golds.pkl'"
     ]
    }
   ],
   "source": [
    "xpos_csv = pd.read_csv(open('./golds/aligned_xpos.csv', 'rb'))\n",
    "xtt_csv =  pd.read_csv(open('./golds/aligned_xtt.csv', 'rb'))\n",
    "top_xpos = get_topk_training_languages(xpos_csv, k=10 )\n",
    "top_xtt = get_topk_training_languages(xtt_csv, k=10)\n",
    "print(top_xpos[\"afr\"])\n",
    "print(top_xtt[\"afr\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61a8492",
   "metadata": {},
   "source": [
    "legacy below this point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9bd45b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_gold_xpos(data):\n",
    "    indices = data.columns \n",
    "    indices = list(indices[1:]) # creates final list of languages\n",
    "    langs_ranked_ties = {}\n",
    "    # creates a ranking for each language (with the language itself removed from consideration)\n",
    "    for language in indices: \n",
    "        values = list(data[language])\n",
    "        values = [-float(a) for a in values]\n",
    "        ranked =  ss.rankdata(values, method = \"ordinal\") #ranks remaining languages (ties are assigned the minimum ranking that would be assigned if no ties were presen)\n",
    "        langs_ranked_ties[language] = (indices, ranked)\n",
    "    return langs_ranked_ties\n",
    "\n",
    "def make_xpos_rank(data, lang):\n",
    "    remove = [lang]\n",
    "    data = data.drop(remove,axis = 1) #columns that match undesirable language codes\n",
    "    data = data.drop(data.loc[data[\"lang_train\"].isin(remove)].index) # removes rows that match undesirable language codes\n",
    "    indices = data.columns \n",
    "    indices = list(indices[1:]) # creates final list of languages\n",
    "    rankings = {}\n",
    "    for lang in indices:\n",
    "        values = list(data[lang])\n",
    "        values = [-float(a) for a in values]\n",
    "        ranked =  ss.rankdata(values, method = \"ordinal\") #ranks remaining languages (ties are assigned the minimum ranking that would be assigned if no ties were presen)\n",
    "        ranked = [i - 1 for i in ranked]\n",
    "        rankings[lang] = ranked\n",
    "    return (indices, rankings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cca641a2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'make_gold_xpos' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m gram_golds \u001b[38;5;241m=\u001b[39m \u001b[43mmake_gold_xpos\u001b[49m(filtered_data)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./training-data/POS_XLMR_gram_golds_no_ties.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      3\u001b[0m     pickle\u001b[38;5;241m.\u001b[39mdump(gram_golds, f)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'make_gold_xpos' is not defined"
     ]
    }
   ],
   "source": [
    "gram_golds = make_gold_xpos(filtered_data)\n",
    "with open(\"./training-data/POS_XLMR_gram_golds_no_ties.pkl\", 'wb') as f:\n",
    "    pickle.dump(gram_golds, f)\n",
    "\n",
    "training = {}\n",
    "for language in filtered_data.columns[1:]:\n",
    "    ranking = make_xpos_rank(filtered_data, language)\n",
    "    training[language] = ranking\n",
    "with open(\"./training-data/POS_XLMR_gram_ranked_train_no_ties.pkl\", 'wb') as f:\n",
    "    pickle.dump(training, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "957042ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtt = pickle.load(open(\"/Users/CitronVert/Desktop/langrank/training-data/POS_gram_golds_no_ties.pkl\", 'rb'))\n",
    "xlmr = pickle.load(open(\"/Users/CitronVert/Desktop/langrank/training-data/POS_XLMR_gram_golds_no_ties.pkl\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d01e2e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "xlmr = {lang: [xlmr[lang][0][i-1] for i in xlmr[lang][1]] for lang in list(xlmr.keys())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ccc57592",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# Function to parse the CSV file\n",
    "def parse_csv(filename):\n",
    "    data = {}\n",
    "    with open(filename, 'r', newline='') as file:\n",
    "        reader = csv.reader(file)\n",
    "        header = next(reader)  # Get header\n",
    "        for row in reader:\n",
    "            test_lang = row[0]\n",
    "            train_langs = header[1:]\n",
    "            accuracies = [float(accuracy) for accuracy in row[1:]]\n",
    "            data[test_lang] = dict(zip(train_langs, accuracies))\n",
    "    return data\n",
    "\n",
    "# Function to get top-3 training languages for each testing language\n",
    "def get_top3_training_languages(data):\n",
    "    top3_training_languages = {}\n",
    "    for test_lang, train_data in data.items():\n",
    "        top_train_langs = sorted(train_data, key=train_data.get, reverse=True)[:3]\n",
    "        top3_training_languages[test_lang] = top_train_langs\n",
    "    return top3_training_languages\n",
    "\n",
    "# Function to write results to a new CSV file\n",
    "def write_to_csv(data, filename):\n",
    "    with open(filename, 'w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['Testing Language', '1st Best Training Language', '2nd Best Training Language', '3rd Best Training Language'])\n",
    "        for test_lang, top_langs in data.items():\n",
    "            writer.writerow([test_lang] + top_langs)\n",
    "\n",
    "\n",
    "input_filename = '/Users/CitronVert/Desktop/langrank/golds/xpos_scores.csv'\n",
    "output_filename = 'best_xlmr_train.csv'\n",
    "\n",
    "# Parse CSV file\n",
    "data = parse_csv(input_filename)\n",
    "\n",
    "# Get top-3 training languages for each testing language\n",
    "top3_training_languages = get_top3_training_languages(data)\n",
    "\n",
    "# Write results to a new CSV file\n",
    "write_to_csv(top3_training_languages, output_filename)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a04b6570",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bel': ['bel', 'hye', 'lit', 'gle', 'hun', 'tam', 'mar'],\n",
       " 'gle': ['lit', 'bel', 'hye', 'gle', 'hun', 'tam', 'mar'],\n",
       " 'hun': ['lit', 'hun', 'bel', 'gle', 'hye', 'tam', 'mar'],\n",
       " 'hye': ['gle', 'tam', 'hun', 'bel', 'hye', 'mar', 'lit'],\n",
       " 'lit': ['gle', 'mar', 'hye', 'hun', 'bel', 'tam', 'lit'],\n",
       " 'mar': ['hun', 'mar', 'gle', 'hye', 'lit', 'bel', 'tam'],\n",
       " 'tam': ['hun', 'mar', 'hye', 'bel', 'lit', 'tam', 'gle']}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xlmr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cf1e7695",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 3, 5, 1, 4, 0, 8, 6, 7]\n",
      "[9, 8, 7, 6, 5, 4, 3, 2, 1]\n",
      "[9, 7, 5, 4, 3, 2, 6, 0, 0]\n",
      "[6, 1, 4, 7, 5, 8, 2, 0, 3]\n",
      "[9, 8, 7, 6, 5, 4, 3, 2, 1]\n",
      "[6, 3, 9, 10, 7, 8, 5, 0, 0]\n",
      "[1, 4, 8, 7, 3, 6, 2, 5, 0]\n",
      "[9, 8, 7, 6, 5, 4, 3, 2, 1]\n",
      "[2, 10, 3, 5, 6, 8, 7, 0, 0]\n",
      "[0, 1, 2, 5, 7, 6, 4, 8, 3]\n",
      "[9, 8, 7, 6, 5, 4, 3, 2, 1]\n",
      "[2, 6, 7, 5, 9, 3, 8, 0, 0]\n",
      "[0, 1, 4, 2, 7, 5, 6, 8, 3]\n",
      "[9, 8, 7, 6, 5, 4, 3, 2, 1]\n",
      "[2, 3, 9, 7, 8, 4, 6, 0, 0]\n",
      "[5, 0, 4, 8, 2, 6, 7, 3, 1]\n",
      "[9, 8, 7, 6, 5, 4, 3, 2, 1]\n",
      "[9, 8, 7, 10, 6, 2, 3, 0, 0]\n",
      "[0, 5, 7, 6, 2, 8, 1, 4, 3]\n",
      "[9, 8, 7, 6, 5, 4, 3, 2, 1]\n",
      "[7, 8, 5, 4, 3, 9, 6, 0, 0]\n",
      "0.7551847984556894\n"
     ]
    }
   ],
   "source": [
    "import sklearn.metrics as sm\n",
    "from statistics import mean\n",
    "\n",
    "def compute_ndcg(lang, ranked_langs, predicted, gamma_max= 9, k=3):\n",
    "    ranking_langs = ranked_langs[lang][0] # list of languages for looking up index in ranking vector\n",
    "    # gives position in ranking based on index (if ranking[0] = 4 then the 0th language [ranking_langs[0]] is the 5th best)\n",
    "    ranking = ranked_langs[lang][1] \n",
    "    print(ranking)\n",
    "    # creates vector to look up the relevance score of a given language by index\n",
    "    scores_by_index = [0] * len(ranking)\n",
    "    for i in range(len(ranking)): \n",
    "        if ranking[i] <= gamma_max:\n",
    "            scores_by_index[i] = gamma_max - (ranking[i] -1)\n",
    "    ideal_score = [i for i in reversed(range(1, gamma_max + 1))] + [0] * (len(ranking) - gamma_max)\n",
    "    predicted_score = [0] * len(ranking)\n",
    "    for j in range(len(predicted)): #for each language in ranking\n",
    "        code = predicted[j]\n",
    "        index = ranking_langs.index(code)\n",
    "        score = scores_by_index[index] #finds the true relevance of each language\n",
    "        predicted_score[j] = score\n",
    "    return sm.ndcg_score(np.asarray([ideal_score]), np.asarray([predicted_score]),k=k)\n",
    "\n",
    "\n",
    "\n",
    "predictions = xlmr\n",
    "rankings = xtt\n",
    "languages = list(predictions.keys())\n",
    "ndcg = {lang: compute_ndcg(lang, rankings, predictions[lang]) for lang in languages}\n",
    "score = str(mean(ndcg.values()))\n",
    "print(score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f33f0a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_df = data.drop(columns=['lang_train']).apply(pd.to_numeric)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "878f7894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column with the greatest span of values: cat\n",
      "cat: Min Value: 25.72064229057845 (Index: san), Max Value: 95.65196362932868 (Index: spa)\n"
     ]
    }
   ],
   "source": [
    "df = data.copy()\n",
    "\n",
    "for column in df.columns:\n",
    "    # Set entries to null where the value in 'lang_train' matches the column name\n",
    "    df.loc[df['lang_train'] == column, column] = np.nan\n",
    "\n",
    "numeric_df = df.drop(columns=['lang_train']).apply(pd.to_numeric)\n",
    "\n",
    "\n",
    "ranges = numeric_df.max(skipna=True) - numeric_df.min(skipna=True)# Find the column with the maximum range\n",
    "column_with_max_span = ranges.idxmax()\n",
    "\n",
    "print(\"Column with the greatest span of values:\", column_with_max_span)\n",
    "min_val = numeric_df[column_with_max_span].min()\n",
    "max_val = numeric_df[column_with_max_span].max()\n",
    "min_index = data.loc[data[column_with_max_span] == min_val, \"lang_train\"].iloc[0]\n",
    "max_index = data.loc[data[column_with_max_span] == max_val, \"lang_train\"].iloc[0]\n",
    "print(f\"{column_with_max_span}: Min Value: {min_val} (lang: {min_index}), Max Value: {max_val} (lang: {max_index})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62fc35d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = data[:54] #cuts off the top 3 data from the bottom of the csv (MT)\n",
    "# data = data[:31] # (DEP)\n",
    "# data = data[:60] # (POS)\n",
    "# data = data[:54] #(EL)\n",
    "\n",
    "\n",
    "def generate_rankings(data, remove):\n",
    "    remove = list(set(remove) & set(data.columns))\n",
    "    data = data.drop(remove, axis = 1) # drop test languages (columns) that match undesirable language codes\n",
    "    data = data.drop(data.loc[data['v extra v \\ trg >>'].isin(remove)].index) # removes rows that match undesirable language codes\n",
    "    indices = data.columns \n",
    "    indices = list(indices[1:]) # creates final list of languages\n",
    "    langs_ranked_ties = {}\n",
    "    # creates a ranking for each language (with the language itself removed from consideration)\n",
    "    for language in indices: \n",
    "        values = list(data[language])\n",
    "        values = [-float(a) for a in values]\n",
    "        ranked =  ss.rankdata(values, method = \"ordinal\") #ranks remaining languages (ties are assigned the minimum ranking that would be assigned if no ties were presen)\n",
    "        langs_ranked_ties[language] = (indices, ranked)\n",
    "    return langs_ranked_ties\n",
    "\n",
    "\n",
    "def generate_ranking(data, language, r):\n",
    "    # remove.append(language)\n",
    "    remove = r.copy()\n",
    "    remove.append(language)\n",
    "    remove = list(set(remove) & set(data.columns))\n",
    "    data = data.drop(remove,axis = 1) #columns that match undesirable language codes\n",
    "    data = data.drop(data.loc[data['v extra v \\ trg >>'].isin(remove)].index) # removes rows that match undesirable language codes\n",
    "    indices = data.columns \n",
    "    indices = list(indices[1:]) # creates final list of languages\n",
    "    langs_ranked_ties = {}\n",
    "    # creates a ranking for each language (with the language itself removed from consideration)\n",
    "    rankings = {}\n",
    "    for lang in indices:\n",
    "        values = list(data[lang])\n",
    "        values = [-float(a) for a in values]\n",
    "        ranked =  ss.rankdata(values, method = \"ordinal\") #ranks remaining languages (ties are assigned the minimum ranking that would be assigned if no ties were presen)\n",
    "        ranked = [i - 1 for i in ranked]\n",
    "        rankings[lang] = ranked\n",
    "    return (indices, rankings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1340c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.rename(columns={'v pivot v \\ test >>':'v extra v \\ trg >>'}) #stupid POS/EL inconsistency \n",
    "# data = data.rename(columns={'Unnamed: 0':'v extra v \\ trg >>'}) #stupid POS/EL inconsistency \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263a9cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finds the intersection of the grambank languages and task languages\n",
    "# finds the rankings for each language with non-intersection languages removed and saves to a pickle file\n",
    "\n",
    "import pickle\n",
    "import gram2vec.lang2vec.lang2vec as l2v\n",
    "\n",
    "gram = l2v.available_gram()\n",
    "\n",
    "\n",
    "col_langs = to_iso(list(data.columns)[1:])\n",
    "print(\"there  were {l} test languages originally. Those languages were {col}.\".format(l = len(col_langs), col = col_langs))\n",
    "\n",
    "lang = list(data['v extra v \\ trg >>'])\n",
    "lang = to_iso(lang)\n",
    "missing = [a for a in col_langs if not a in lang]\n",
    "print(\"there were  {l} languages excluded for not being present in ranking data. Those languages were {missing}.\".format(l = len(missing), missing = missing))\n",
    "\n",
    "\n",
    "data['v extra v \\ trg >>'] = lang #reassigns ranking langs to ISO-3 codes\n",
    "data.columns = ['v extra v \\ trg >>'] + col_langs #reassigns test languages to ISO-3 codes\n",
    "data = data.drop(missing,axis = 1) #drop test languages (columns) that aren't present in ranking languages (for now)\n",
    "\n",
    "col_langs = list(data.columns)[1:]  #reassigns column headers to remaining test languages\n",
    "print(\"After dropping missing languages, there are {l} test languages remaining. Those languages were {col}.\".format(l = len(col_langs), col = col_langs))\n",
    "\n",
    "\n",
    "common_test = (set(gram) & set(col_langs)) # final set of test languages (in grambank AND in ranking languages)\n",
    "\n",
    "total_langs = lang + col_langs\n",
    "\n",
    "common= (set(gram) & set(total_langs))\n",
    "remove = list(set(total_langs) ^ common) # languages that should be removed (not in grambank)\n",
    "print(\"there were {l} total languages excluded for not being present in grambank. Those languages were {missing}.\".format(l = len(remove), missing = remove))\n",
    "\n",
    "training = {}\n",
    "gram_golds = generate_rankings(data, remove)\n",
    "with open(\"./training-data/MT_gram_golds_no_ties.pkl\", 'wb') as f:\n",
    "    pickle.dump(gram_golds, f)\n",
    "\n",
    "for language in common_test:\n",
    "    ranking = generate_ranking(data, language, remove)\n",
    "    training[language] = ranking\n",
    "with open(\"./training-data/MT_ranked_train_no_ties.pkl\", 'wb') as f:\n",
    "    pickle.dump(training, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loads in list of languages that have precomputed distances in lang2vec\n",
    "with open(\"./gram2vec/lang2vec/data/distances/distances_languages.txt\") as f:\n",
    "    dist_langs = f.read()\n",
    "dist_langs =  dist_langs.split(\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1268fb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract ISO6393 codes from... somewhere\n",
    "with open('/Users/CitronVert/Downloads/resourcemap.json') as f:\n",
    "  \n",
    "# returns JSON object as \n",
    "# a dictionary\n",
    "    data = json.load(f)\n",
    "\n",
    "def get_iso6393(identifiers):\n",
    "    for identifier in identifiers:\n",
    "        if identifier.get(\"type\") == \"iso639-3\":\n",
    "            return identifier.get(\"identifier\")\n",
    "    return None\n",
    "\n",
    "# Use list comprehension to extract the required fields\n",
    "glotto2iso = {\n",
    "        resource.get(\"id\"): get_iso6393(resource.get(\"identifiers\", []))\n",
    "        for resource in data.get(\"resources\", [])\n",
    "        if get_iso6393(resource.get(\"identifiers\", [])) is not None\n",
    "    }\n",
    "\n",
    "iso2glotto = {v: k for k, v in glotto2iso.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6265a4a-d4d6-4b23-9060-0c957307f74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace language codes in the vector data with ISO codes\n",
    "\n",
    "from pycldf.dataset import Dataset\n",
    "# metadata = \"/Users/CitronVert/Desktop/grambank-grambank-analysed-fcf971a/grambank/cldf/StructureDataset-metadata.json\"\n",
    "imputed = \"/Users/CitronVert/Desktop/NALA/grambank-grambank-analysed-fcf971a/R_grambank/output/GB_wide/GB_wide_imputed_binarized.tsv\"\n",
    "v = pd.read_csv(imputed, sep='\\t')\n",
    "for lang in v[\"Language_ID\"]:\n",
    "    if lang in glotto2iso.keys():\n",
    "        v.loc[ v[\"Language_ID\"] == lang, \"Language_ID\"] = glotto2iso[lang]\n",
    "    else:\n",
    "        v.drop(v[v['Language_ID'] == lang].index, inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd80415b",
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = list(v.columns[1:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a47c4d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = \"/Users/CitronVert/Downloads/feature_grouping_with_alternatives.csv\"\n",
    "gram_categories = pd.read_csv(categories)\n",
    "cats = {}\n",
    "groups = {}\n",
    "for feat in feats:\n",
    "    if not len(feat) == 5:\n",
    "        feat = feat[:5]\n",
    "    domain = gram_categories.loc[gram_categories[\"Feature_ID\"] == feat, 'Main_domain'].item()\n",
    "    fine = gram_categories.loc[gram_categories[\"Feature_ID\"] == feat, 'Finer_grouping'].item()\n",
    "    if fine in groups.keys():\n",
    "        groups[fine].append(feat)\n",
    "    else:\n",
    "        groups[fine] = [feat]\n",
    "    if domain in cats.keys():\n",
    "        cats[domain].append(feat)\n",
    "    else:\n",
    "        cats[domain] = [feat]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6de78163-1e92-4381-93cf-ea93430c489f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data: pandas array with grambank vector data (imputed and converted to iso codes)\n",
    "#iso: string ISO code for a language\n",
    "#reuturns the grambank feature vector\n",
    "def get_vector(data, iso, sub_feats=None):\n",
    "    uriel = np.array(l2v.get_features(iso, \"syntax_knn\", sub_feats=sub_feats)[iso])\n",
    "    # gram = np.array(data[data['Language_ID'] == iso].values.tolist()[0][1:])\n",
    "    # concat = np.concatenate((uriel, gram))\n",
    "    return uriel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e54db655",
   "metadata": {},
   "outputs": [],
   "source": [
    "#finds the languages that are present in both lang2vec and grambank and reports the length\n",
    "with open(\"/Users/CitronVert/Desktop/NALA/bankrank/gram2vec/lang2vec/data/distances/intersection_langs.txt\", 'r') as file:\n",
    "    intersection = file.read()\n",
    "intersection = intersection.split(\",\")\n",
    "d =len(intersection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "209cec48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in csv\n",
    "fine_cat =\"/Users/CitronVert/Desktop/NALA/bankrank data/fine_categorization.csv\"\n",
    "cats = pd.read_csv(fine_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb3ea497",
   "metadata": {},
   "outputs": [],
   "source": [
    "np_order = cats[\"Uriel\"][12].split(\",\")\n",
    "clause = cats[\"Uriel\"][11].split(\",\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "90ecde2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "41\n"
     ]
    }
   ],
   "source": [
    "np_order=[a.strip() for a in np_order]\n",
    "clause = [a.strip() for a in clause]\n",
    "\n",
    "print(len(np_order))\n",
    "print(len(clause))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_vecs = [get_vector(v, iso) for iso in intersection]\n",
    "sub_np_feat_vecs = [get_vector(v, iso, np_order) for iso in intersection]\n",
    "sub_clause_feat_vecs = [get_vector(v, iso, clause) for iso in intersection]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 0.,\n",
       "       0., 1., 0., 1., 1., 1., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 1.,\n",
       "       0., 0., 0., 1., 0., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0., 1., 1.,\n",
       "       0., 0.])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub_np_feat_vecs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 1., 0., 1., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1., 0.,\n",
       "       0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "       0., 0., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 0., 0., 1., 0., 0.,\n",
       "       1., 1., 1., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0.,\n",
       "       1., 0., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1.,\n",
       "       0.])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat_vecs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "08f32033",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_arr = np.stack(feat_vecs)\n",
    "sub_np_feat_arr =  np.stack(sub_np_feat_vecs)\n",
    "sub_clause_feat_arr =  np.stack(sub_clause_feat_vecs)\n",
    "\n",
    "# print(sub_np_feat_arr.shape)\n",
    "# print(sub_clause_feat_arr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "401b7e18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "import sklearn.metrics.pairwise as sk\n",
    "import scipy.sparse as sparse\n",
    "\n",
    "cosine_similarities = sk.cosine_similarity(feat_arr)\n",
    "knn = sparse.csr_matrix(1-cosine_similarities)\n",
    "sparse.save_npz(\"./URIEL_knn_distances.npz\", knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse as sparse\n",
    "\n",
    "np = sparse.load_npz(\"/Users/CitronVert/Desktop/NALA/bankrank/gram2vec/lang2vec/data/distances/URIEL_knn_distances_np_order_ablation.npz\")\n",
    "clause = sparse.load_npz(\"/Users/CitronVert/Desktop/NALA/bankrank/gram2vec/lang2vec/data/distances/URIEL_knn_distances_clause_ablation.npz\")\n",
    "reg = sparse.load_npz(\"/Users/CitronVert/Desktop/NALA/bankrank/gram2vec/lang2vec/data/distances/URIEL_knn_distances.npz\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t-2.220446049250313e-16\n",
      "  (0, 1)\t0.5890025317366068\n",
      "  (0, 2)\t0.05370837673721851\n",
      "  (0, 3)\t0.4856555001263603\n",
      "  (0, 4)\t0.3043344070000654\n",
      "  (0, 5)\t0.48550424457247354\n",
      "  (0, 6)\t0.3617152614957745\n",
      "  (0, 7)\t0.4645503299035949\n",
      "  (0, 8)\t0.6284197321912309\n",
      "  (0, 9)\t0.41666666666666663\n",
      "  (0, 10)\t0.5068030380839282\n",
      "  (0, 11)\t0.3036893761772086\n",
      "  (0, 12)\t0.06840573860297583\n",
      "  (0, 13)\t0.34927599213080796\n",
      "  (0, 14)\t0.5628071751074061\n",
      "  (0, 15)\t0.452003375648809\n",
      "  (0, 16)\t0.571253537143728\n",
      "  (0, 17)\t0.4246035444312495\n",
      "  (0, 18)\t0.6712020253892854\n",
      "  (0, 19)\t0.2602045571258924\n",
      "  (0, 20)\t0.04100590738541632\n",
      "  (0, 21)\t0.38888888888888873\n",
      "  (0, 22)\t0.6055946811266921\n",
      "  (0, 23)\t0.09850212828958171\n",
      "  (0, 24)\t0.5463035642189196\n",
      "  :\t:\n",
      "  (1469, 1445)\t0.4997835966139753\n",
      "  (1469, 1446)\t0.5454545454545454\n",
      "  (1469, 1447)\t0.4848725736710717\n",
      "  (1469, 1448)\t0.4276361929678575\n",
      "  (1469, 1449)\t0.4197411468143405\n",
      "  (1469, 1450)\t0.4997835966139753\n",
      "  (1469, 1451)\t0.5067799747921895\n",
      "  (1469, 1452)\t0.28454524120982194\n",
      "  (1469, 1453)\t0.6873473002596386\n",
      "  (1469, 1454)\t0.5691797815723354\n",
      "  (1469, 1455)\t0.5320903998212967\n",
      "  (1469, 1456)\t0.4777670321329064\n",
      "  (1469, 1457)\t0.4153154178481695\n",
      "  (1469, 1458)\t0.4276361929678575\n",
      "  (1469, 1459)\t0.4770422115649787\n",
      "  (1469, 1460)\t0.5357929174514724\n",
      "  (1469, 1461)\t0.6376284623302606\n",
      "  (1469, 1462)\t0.5357929174514724\n",
      "  (1469, 1463)\t0.3036893761772086\n",
      "  (1469, 1464)\t0.6228317454293213\n",
      "  (1469, 1465)\t0.6046522536925664\n",
      "  (1469, 1466)\t0.606060606060606\n",
      "  (1469, 1467)\t0.44093460798032524\n",
      "  (1469, 1468)\t0.5123393477776219\n",
      "  (1469, 1469)\t-2.220446049250313e-16\n"
     ]
    }
   ],
   "source": [
    "print(reg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71255ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "GRAMBANK_LANGUAGES = l2v.available_grambank()\n",
    "GRAMBANK_DISTANCES =  \"/Users/CitronVert/Desktop/langrank/gram2vec/lang2vec/data/learned.npy\"\n",
    "with open(GRAMBANK_DISTANCES, \"rb\") as f:\n",
    "    feat_pred = np.load(GRAMBANK_DISTANCES, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc07ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718b3f4c-f4a4-438d-a0b5-5e26964b288c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for every pair of languages that is present in both grambank and lang2vec,\n",
    "#find the cosine difference between their feature vectors\n",
    "import scipy.sparse as sparse\n",
    "\n",
    "res = np.zeros((d,d))\n",
    "for x in range(d):\n",
    "    l1 = intersection[x]\n",
    "    v1 = get_vector(v, l1)\n",
    "    for y in range(x,d):\n",
    "        l2 = intersection[y]\n",
    "        v2 = get_vector(v, l2)\n",
    "        diff = spatial.distance.cosine(v1, v2)\n",
    "        res[x][y] = diff\n",
    "\n",
    "\n",
    "# #saves the distance measures as a sparse matrix (for compatability with lang2vec)\n",
    "sA = sparse.csr_matrix(res)\n",
    "sparse.save_npz(\"./concat_distances.npz\", sA)\n",
    "               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7cc9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "intersection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b22cc3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = intersection.index(\"mij\")\n",
    "y = intersection.index(\"sed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4bbc757",
   "metadata": {},
   "outputs": [],
   "source": [
    "res[y][x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1f1181-537a-4c0d-8c20-cd3da803cdd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#saves a list of the languages for which we have distance values\n",
    "data_string = \",\".join(str(item) for item in intersection)\n",
    "\n",
    "    # Open the file in write mode\n",
    "with open(\"./URIEL_distance_langs.txt\", 'w') as file:\n",
    "    # Write the data string to the file\n",
    "    file.write(data_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96922447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training code for reference\n",
    "# def test_train():\n",
    "#     langs = [\"aze\", \"ben\", \"fin\"]\n",
    "#     datasets = [os.path.join(root, \"sample-data\", \"ted-train.orig.{}\".format(l)) for l in langs]\n",
    "#     seg_datasets = [os.path.join(root, \"sample-data\", \"ted-train.orig.spm8000.{}\".format(l)) for l in langs]\n",
    "#     rank = [[0, 1, 2], [1, 0, 2], [2, 1, 0]] # random\n",
    "#     tmp_dir = \"tmp\"\n",
    "#     prepare_train_file(datasets=datasets, segmented_datasets=seg_datasets,\n",
    "#                        langs=langs, rank=rank, tmp_dir=tmp_dir, task=\"MT\")\n",
    "#     output_model = \"{}/model.txt\".format(tmp_dir)\n",
    "#     train(tmp_dir=tmp_dir, output_model=output_model)\n",
    "#     assert os.path.isfile(output_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a3c340",
   "metadata": {},
   "outputs": [],
   "source": [
    "TED_PATH = \"/Users/CitronVert/Desktop/langrank/indexed/MT/ted.npy\"\n",
    "def create_MT_dataset(data_path, lang): \n",
    "    data = np.load(data_path, encoding='latin1', allow_pickle=True).item()\n",
    "    filename = \"ted-train.orig.\"+lang\n",
    "    \n",
    "\n",
    "\n",
    "def train(langs, datasets, seg_datasets, rank): \n",
    "    tmp_dir = \"tmp\"\n",
    "    prepare_train_file(datasets=datasets, segmented_datasets=seg_datasets,\n",
    "                       langs=langs, rank=rank, tmp_dir=tmp_dir, task=\"MT\")\n",
    "    output_model = \"{}/model.txt\".format(tmp_dir)\n",
    "    train(tmp_dir=tmp_dir, output_model=output_model)\n",
    "    assert os.path.isfile(output_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015e5b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to compute the ndcg score\n",
    "#lang: ISO code for task language\n",
    "#ranked_langs: gold rankings for task\n",
    "#predicted: predicted ranking for lang\n",
    "#gamma_max: hyper param from lin et al.\n",
    "\n",
    "import sklearn.metrics as sm\n",
    "import numpy as np\n",
    "def compute_ndcg(lang, ranked_langs, predicted, gamma_max= 10):\n",
    "    ranking_langs = ranked_langs[lang][0] # list of languages for looking up index in ranking vector\n",
    "    # gives position in ranking based on index (if ranking[0] = 4 then the 0th language [ranking_langs[0]] is the 4th best)\n",
    "    ranking = ranked_langs[lang][1] \n",
    "    # creates vector to look up the relevance score of a given language by index\n",
    "    scores_by_index = [0] * len(ranking)\n",
    "    for i in range(len(ranking)): \n",
    "        if ranking[i] <= gamma_max:\n",
    "            scores_by_index[i] = gamma_max - (ranking[i] - 1)\n",
    "    print(\"scores by index\")\n",
    "    print(scores_by_index)\n",
    "    ideal_score = [i for i in reversed(range(1, gamma_max + 1))] + [0] * (len(ranking) - gamma_max)\n",
    "    print(\"IDEAL\")\n",
    "    print(ideal_score)\n",
    "    predicted_score = [0] * len(ranking)\n",
    "    for j in range(len(predicted)): #for each language in ranking\n",
    "        code = predicted[j]\n",
    "        print(code)\n",
    "        index = ranking_langs.index(code)\n",
    "        print(ranking[index])\n",
    "        score = scores_by_index[index] #finds the true relevance of each language\n",
    "        predicted_score[j] = score\n",
    "    return sm.ndcg_score(np.asarray([ideal_score]), np.asarray([predicted_score]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08ba17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a small ndcg test\n",
    "# remember-- we don't exclude the test language\n",
    "dummy = {\n",
    "    \"aze\": ([\"ben\", \"fin\", \"deu\", \"eng\"],[1, 2, 3, 4]),\n",
    "    \"ben\": ([\"aze\",\"fin\", \"deu\", \"eng\"],[1, 3, 2, 4]),\n",
    "    \"fin\": ([\"aze\", \"ben\", \"deu\", \"eng\"],[3, 4, 2, 1]),\n",
    "    \"deu\": ([\"aze\", \"ben\", \"fin\", \"eng\"],[4, 3, 2, 1]),\n",
    "    \"eng\": ([\"aze\", \"ben\", \"fin\", \"deu\"],[4, 3, 2, 1])\n",
    "}\n",
    "lang = \"eng\"\n",
    "predicted = [\"fin\",\"deu\",\"aze\",\"ben\"]\n",
    "gamma_max = 2 \n",
    "print(compute_ndcg(lang, dummy, predicted, gamma_max))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b494daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify the code to accept params instead of freakin data\n",
    "# pull list of MT languages from ... somewhere\n",
    "# run write a function to run langrank_predict with the all model for all langs in list and return ndcg\n",
    "# average list\n",
    "# is it same??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a68d458",
   "metadata": {},
   "outputs": [],
   "source": [
    "import langrank as lr\n",
    "import pickle\n",
    "with open(\"./MT_ranked_ties.pkl\", 'rb') as f:\n",
    "    rankings = pickle.load(f)\n",
    "languages = list(rankings.keys())\n",
    "\n",
    "predicted = {}\n",
    "for lang in languages:\n",
    "    prepared = lr.prepare_featureset(lang)\n",
    "    predicted[lang] = lr.rank(prepared, task=\"MT\", candidates=\"all\", return_langs = True, uriel = True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff88c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./MT_full_ranked_no_ties.pkl\", 'rb') as f:\n",
    "    rankings = pickle.load(f)\n",
    "languages = list(rankings.keys())\n",
    "rank = rankings[\"aze\"][1]\n",
    "gold = [0]*len(languages)\n",
    "for i in range(len(languages)):\n",
    "    gold[rank[i]-1] = languages[i]\n",
    "print(gold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07f2224",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"./uriel_predictions.pkl\", 'rb') as f:\n",
    "    predictions = pickle.load(f)\n",
    "print(predictions[\"aze\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d235744",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./MT_full_ranked_ties.pkl\", 'rb') as f:\n",
    "    rankings = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b298398",
   "metadata": {},
   "outputs": [],
   "source": [
    "lang = \"aze\"\n",
    "ranking_langs = rankings[lang][0] # list of languages for looking up index in ranking vector\n",
    "ranking = rankings[lang][1] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef59a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "langs = [\"\"] * len(ranking_langs)\n",
    "for i in range(len(ranking_langs)):\n",
    "    langs[ranking[i] - 1] = ranking_langs[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518262ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "langs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053616e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d204875",
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = {\"MT\", \"EL\", \"DEP\", \"POS\"}\n",
    "cases = {\"uriel\", \"gram\"}\n",
    "path = \"/Users/CitronVert/Desktop/langrank/results\"\n",
    "for task in tasks:\n",
    "    for case in cases:\n",
    "        p = \"{base}/{task}/{task}_{case}_predictions.pkl\".format(base = path, task = task, case = case)\n",
    "        with open(p, 'rb') as f:\n",
    "            predictions = pickle.load(f)\n",
    "        n = \"{base}/{task}/{task}_{case}_ndcg.pkl\".format(base = path, task = task, case = case)\n",
    "        with open(n, 'rb') as f:\n",
    "            scores = pickle.load(f)\n",
    "        vals = [i for i in list(scores.values())]\n",
    "        # vals\n",
    "        predict = pd.DataFrame.from_dict(predictions)\n",
    "        scores_d = pd.DataFrame( columns = list(scores.keys()))\n",
    "        scores_d.loc[len(scores_d)]= vals \n",
    "        predict.to_csv(r\"{base}/{task}/{task}_{case}_predictions.csv\".format(base = path, task = task, case = case))\n",
    "        scores_d.to_csv(r\"{base}/{task}/{task}_{case}_ndcg.csv\".format(base = path, task = task, case = case))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effe664d",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_d.loc[len(scores_d)]= vals "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34dfb259",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def percent_missing(lang, path):\n",
    "    v = pd.read_csv(path, sep='\\t')\n",
    "    glotto = iso2glotto[lang]\n",
    "    vector = list(get_vector(v, glotto))\n",
    "    a = sum(math.isnan(x) for x in vector)\n",
    "    return a/len(vector)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a06446e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_task_langs(task):\n",
    "    with open(\"./training-data/{}_ranked_train_no_ties.pkl\".format(task), 'rb') as f:\n",
    "        rankings = pickle.load(f)\n",
    "    languages = list(rankings.keys())\n",
    "    return languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d2a444",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = \"/Users/CitronVert/Desktop/grambank-grambank-analysed-fcf971a/R_grambank/output/GB_wide/GB_cropped_for_missing.tsv\"\n",
    "tasks = [\"MT\", \"DEP\", \"POS\", \"EL\"]\n",
    "for task in tasks:\n",
    "    d = {}\n",
    "    langs = get_task_langs(task)\n",
    "    for lang in langs: \n",
    "        missing = percent_missing(lang, PATH)\n",
    "        d[lang] = missing\n",
    "    with open(r\"./{task}_percent_missing.txt\".format(task = task), 'w') as fp:\n",
    "        for item in d.keys():\n",
    "            percent = d[item]\n",
    "            # write each item on a new line\n",
    "            fp.write(\"{lang}: {percent}\\n\".format(lang = item, percent = percent))\n",
    "        print('Done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487a95db",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'./uriel_features.txt', 'w') as fp:\n",
    "    for item in features:\n",
    "        # write each item on a new line\n",
    "        fp.write(\"%s\\n\" % item)\n",
    "    print('Done')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "grambank",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
